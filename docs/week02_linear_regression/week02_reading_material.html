<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dr.&nbsp;Raktim Bhattacharya">

<title>Linear Regression: Reading Material</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</a></li>
  </ul></li>
  <li><a href="#motivation-the-fuel-crisis-challenge" id="toc-motivation-the-fuel-crisis-challenge" class="nav-link" data-scroll-target="#motivation-the-fuel-crisis-challenge"><span class="header-section-number">2</span> Motivation: The Fuel Crisis Challenge</a>
  <ul class="collapse">
  <li><a href="#the-100-million-question" id="toc-the-100-million-question" class="nav-link" data-scroll-target="#the-100-million-question"><span class="header-section-number">2.1</span> The $100 Million Question</a></li>
  <li><a href="#real-impact" id="toc-real-impact" class="nav-link" data-scroll-target="#real-impact"><span class="header-section-number">2.2</span> Real Impact</a></li>
  </ul></li>
  <li><a href="#from-wind-tunnel-to-flight-the-data-challenge" id="toc-from-wind-tunnel-to-flight-the-data-challenge" class="nav-link" data-scroll-target="#from-wind-tunnel-to-flight-the-data-challenge"><span class="header-section-number">3</span> From Wind Tunnel to Flight: The Data Challenge</a>
  <ul class="collapse">
  <li><a href="#traditional-approach-empirical-models" id="toc-traditional-approach-empirical-models" class="nav-link" data-scroll-target="#traditional-approach-empirical-models"><span class="header-section-number">3.1</span> Traditional Approach: Empirical Models</a></li>
  <li><a href="#available-data-sources" id="toc-available-data-sources" class="nav-link" data-scroll-target="#available-data-sources"><span class="header-section-number">3.2</span> Available Data Sources</a>
  <ul class="collapse">
  <li><a href="#wind-tunnel-data" id="toc-wind-tunnel-data" class="nav-link" data-scroll-target="#wind-tunnel-data"><span class="header-section-number">3.2.1</span> 1. Wind Tunnel Data</a></li>
  <li><a href="#flight-test-data" id="toc-flight-test-data" class="nav-link" data-scroll-target="#flight-test-data"><span class="header-section-number">3.2.2</span> 2. Flight Test Data</a></li>
  <li><a href="#operational-data" id="toc-operational-data" class="nav-link" data-scroll-target="#operational-data"><span class="header-section-number">3.2.3</span> 3. Operational Data</a></li>
  </ul></li>
  <li><a href="#the-linear-regression-framework" id="toc-the-linear-regression-framework" class="nav-link" data-scroll-target="#the-linear-regression-framework"><span class="header-section-number">3.3</span> The Linear Regression Framework</a></li>
  </ul></li>
  <li><a href="#mathematical-foundation-the-linear-model" id="toc-mathematical-foundation-the-linear-model" class="nav-link" data-scroll-target="#mathematical-foundation-the-linear-model"><span class="header-section-number">4</span> Mathematical Foundation: The Linear Model</a>
  <ul class="collapse">
  <li><a href="#general-form" id="toc-general-form" class="nav-link" data-scroll-target="#general-form"><span class="header-section-number">4.1</span> General Form</a></li>
  <li><a href="#vector-notation" id="toc-vector-notation" class="nav-link" data-scroll-target="#vector-notation"><span class="header-section-number">4.2</span> Vector Notation</a></li>
  </ul></li>
  <li><a href="#matrix-formulation" id="toc-matrix-formulation" class="nav-link" data-scroll-target="#matrix-formulation"><span class="header-section-number">5</span> Matrix Formulation</a>
  <ul class="collapse">
  <li><a href="#design-matrix-structure" id="toc-design-matrix-structure" class="nav-link" data-scroll-target="#design-matrix-structure"><span class="header-section-number">5.1</span> Design Matrix Structure</a></li>
  <li><a href="#aerospace-example-drag-prediction" id="toc-aerospace-example-drag-prediction" class="nav-link" data-scroll-target="#aerospace-example-drag-prediction"><span class="header-section-number">5.2</span> Aerospace Example: Drag Prediction</a></li>
  </ul></li>
  <li><a href="#key-insight-what-linear-means" id="toc-key-insight-what-linear-means" class="nav-link" data-scroll-target="#key-insight-what-linear-means"><span class="header-section-number">6</span> Key Insight: What “Linear” Means</a>
  <ul class="collapse">
  <li><a href="#linear-in-parameters-not-features" id="toc-linear-in-parameters-not-features" class="nav-link" data-scroll-target="#linear-in-parameters-not-features"><span class="header-section-number">6.1</span> Linear in Parameters, Not Features</a></li>
  <li><a href="#why-linearity-in-parameters-matters" id="toc-why-linearity-in-parameters-matters" class="nav-link" data-scroll-target="#why-linearity-in-parameters-matters"><span class="header-section-number">6.2</span> Why Linearity in Parameters Matters</a></li>
  <li><a href="#aerospace-example" id="toc-aerospace-example" class="nav-link" data-scroll-target="#aerospace-example"><span class="header-section-number">6.3</span> Aerospace Example</a></li>
  </ul></li>
  <li><a href="#real-data-noisy-measurements" id="toc-real-data-noisy-measurements" class="nav-link" data-scroll-target="#real-data-noisy-measurements"><span class="header-section-number">7</span> Real Data: Noisy Measurements</a>
  <ul class="collapse">
  <li><a href="#understanding-measurement-noise" id="toc-understanding-measurement-noise" class="nav-link" data-scroll-target="#understanding-measurement-noise"><span class="header-section-number">7.1</span> Understanding Measurement Noise</a></li>
  <li><a href="#characteristics-of-real-wind-tunnel-data" id="toc-characteristics-of-real-wind-tunnel-data" class="nav-link" data-scroll-target="#characteristics-of-real-wind-tunnel-data"><span class="header-section-number">7.2</span> Characteristics of Real Wind Tunnel Data</a></li>
  <li><a href="#why-the-error-term-epsilon_i-is-essential" id="toc-why-the-error-term-epsilon_i-is-essential" class="nav-link" data-scroll-target="#why-the-error-term-epsilon_i-is-essential"><span class="header-section-number">7.3</span> Why the Error Term <span class="math inline">\(\epsilon_i\)</span> is Essential</a></li>
  </ul></li>
  <li><a href="#the-optimization-problem" id="toc-the-optimization-problem" class="nav-link" data-scroll-target="#the-optimization-problem"><span class="header-section-number">8</span> The Optimization Problem</a>
  <ul class="collapse">
  <li><a href="#objective-minimize-squared-error" id="toc-objective-minimize-squared-error" class="nav-link" data-scroll-target="#objective-minimize-squared-error"><span class="header-section-number">8.1</span> Objective: Minimize Squared Error</a></li>
  <li><a href="#why-square-the-errors" id="toc-why-square-the-errors" class="nav-link" data-scroll-target="#why-square-the-errors"><span class="header-section-number">8.2</span> Why Square the Errors?</a></li>
  <li><a href="#matrix-form" id="toc-matrix-form" class="nav-link" data-scroll-target="#matrix-form"><span class="header-section-number">8.3</span> Matrix Form</a></li>
  <li><a href="#the-optimization-goal" id="toc-the-optimization-goal" class="nav-link" data-scroll-target="#the-optimization-goal"><span class="header-section-number">8.4</span> The Optimization Goal</a></li>
  </ul></li>
  <li><a href="#derivation-normal-equations" id="toc-derivation-normal-equations" class="nav-link" data-scroll-target="#derivation-normal-equations"><span class="header-section-number">9</span> Derivation: Normal Equations</a>
  <ul class="collapse">
  <li><a href="#step-1-expand-the-objective-function" id="toc-step-1-expand-the-objective-function" class="nav-link" data-scroll-target="#step-1-expand-the-objective-function"><span class="header-section-number">9.1</span> Step 1: Expand the Objective Function</a></li>
  <li><a href="#step-2-take-derivative-with-respect-to-boldsymbolbeta" id="toc-step-2-take-derivative-with-respect-to-boldsymbolbeta" class="nav-link" data-scroll-target="#step-2-take-derivative-with-respect-to-boldsymbolbeta"><span class="header-section-number">9.2</span> Step 2: Take Derivative with Respect to <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
  <li><a href="#step-3-set-to-zero-and-solve" id="toc-step-3-set-to-zero-and-solve" class="nav-link" data-scroll-target="#step-3-set-to-zero-and-solve"><span class="header-section-number">9.3</span> Step 3: Set to Zero and Solve</a></li>
  <li><a href="#step-4-closed-form-solution" id="toc-step-4-closed-form-solution" class="nav-link" data-scroll-target="#step-4-closed-form-solution"><span class="header-section-number">9.4</span> Step 4: Closed-Form Solution</a></li>
  </ul></li>
  <li><a href="#geometric-interpretation-understanding-the-error" id="toc-geometric-interpretation-understanding-the-error" class="nav-link" data-scroll-target="#geometric-interpretation-understanding-the-error"><span class="header-section-number">10</span> Geometric Interpretation: Understanding the Error</a>
  <ul class="collapse">
  <li><a href="#what-is-the-residual-vector" id="toc-what-is-the-residual-vector" class="nav-link" data-scroll-target="#what-is-the-residual-vector"><span class="header-section-number">10.1</span> What is the Residual Vector?</a></li>
  <li><a href="#goal-minimize-the-length-of-the-error-vector" id="toc-goal-minimize-the-length-of-the-error-vector" class="nav-link" data-scroll-target="#goal-minimize-the-length-of-the-error-vector"><span class="header-section-number">10.2</span> Goal: Minimize the Length of the Error Vector</a></li>
  <li><a href="#column-space-of-boldsymbolx" id="toc-column-space-of-boldsymbolx" class="nav-link" data-scroll-target="#column-space-of-boldsymbolx"><span class="header-section-number">10.3</span> Column Space of <span class="math inline">\(\boldsymbol{X}\)</span></a></li>
  <li><a href="#the-fundamental-geometric-principle" id="toc-the-fundamental-geometric-principle" class="nav-link" data-scroll-target="#the-fundamental-geometric-principle"><span class="header-section-number">10.4</span> The Fundamental Geometric Principle</a></li>
  </ul></li>
  <li><a href="#why-projection-minimizes-error" id="toc-why-projection-minimizes-error" class="nav-link" data-scroll-target="#why-projection-minimizes-error"><span class="header-section-number">11</span> Why Projection Minimizes Error</a>
  <ul class="collapse">
  <li><a href="#visualizing-the-projection" id="toc-visualizing-the-projection" class="nav-link" data-scroll-target="#visualizing-the-projection"><span class="header-section-number">11.1</span> Visualizing the Projection</a></li>
  <li><a href="#why-perpendicular-is-optimal" id="toc-why-perpendicular-is-optimal" class="nav-link" data-scroll-target="#why-perpendicular-is-optimal"><span class="header-section-number">11.2</span> Why Perpendicular is Optimal</a></li>
  <li><a href="#mathematical-statement-of-orthogonality" id="toc-mathematical-statement-of-orthogonality" class="nav-link" data-scroll-target="#mathematical-statement-of-orthogonality"><span class="header-section-number">11.3</span> Mathematical Statement of Orthogonality</a></li>
  </ul></li>
  <li><a href="#deriving-the-optimal-solution-via-projection" id="toc-deriving-the-optimal-solution-via-projection" class="nav-link" data-scroll-target="#deriving-the-optimal-solution-via-projection"><span class="header-section-number">12</span> Deriving the Optimal Solution via Projection</a>
  <ul class="collapse">
  <li><a href="#step-1-state-the-orthogonality-condition" id="toc-step-1-state-the-orthogonality-condition" class="nav-link" data-scroll-target="#step-1-state-the-orthogonality-condition"><span class="header-section-number">12.1</span> Step 1: State the Orthogonality Condition</a></li>
  <li><a href="#step-2-express-in-terms-of-boldsymbolbeta" id="toc-step-2-express-in-terms-of-boldsymbolbeta" class="nav-link" data-scroll-target="#step-2-express-in-terms-of-boldsymbolbeta"><span class="header-section-number">12.2</span> Step 2: Express in Terms of <span class="math inline">\(\boldsymbol{\beta}\)</span></a></li>
  <li><a href="#step-3-derive-the-normal-equations" id="toc-step-3-derive-the-normal-equations" class="nav-link" data-scroll-target="#step-3-derive-the-normal-equations"><span class="header-section-number">12.3</span> Step 3: Derive the Normal Equations</a></li>
  <li><a href="#step-4-solve-for-boldsymbolbetaast" id="toc-step-4-solve-for-boldsymbolbetaast" class="nav-link" data-scroll-target="#step-4-solve-for-boldsymbolbetaast"><span class="header-section-number">12.4</span> Step 4: Solve for <span class="math inline">\(\boldsymbol{\beta}^\ast\)</span></a></li>
  </ul></li>
  <li><a href="#the-projection-matrix" id="toc-the-projection-matrix" class="nav-link" data-scroll-target="#the-projection-matrix"><span class="header-section-number">13</span> The Projection Matrix</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition"><span class="header-section-number">13.1</span> Definition</a></li>
  <li><a href="#computing-predictions" id="toc-computing-predictions" class="nav-link" data-scroll-target="#computing-predictions"><span class="header-section-number">13.2</span> Computing Predictions</a></li>
  <li><a href="#key-properties" id="toc-key-properties" class="nav-link" data-scroll-target="#key-properties"><span class="header-section-number">13.3</span> Key Properties</a>
  <ul class="collapse">
  <li><a href="#idempotent-projecting-twice-projecting-once" id="toc-idempotent-projecting-twice-projecting-once" class="nav-link" data-scroll-target="#idempotent-projecting-twice-projecting-once"><span class="header-section-number">13.3.1</span> 1. Idempotent (Projecting Twice = Projecting Once)</a></li>
  <li><a href="#symmetric" id="toc-symmetric" class="nav-link" data-scroll-target="#symmetric"><span class="header-section-number">13.3.2</span> 2. Symmetric</a></li>
  <li><a href="#projects-onto-colboldsymbolx" id="toc-projects-onto-colboldsymbolx" class="nav-link" data-scroll-target="#projects-onto-colboldsymbolx"><span class="header-section-number">13.3.3</span> 3. Projects onto col(<span class="math inline">\(\boldsymbol{X}\)</span>)</a></li>
  <li><a href="#residual-matrix" id="toc-residual-matrix" class="nav-link" data-scroll-target="#residual-matrix"><span class="header-section-number">13.3.4</span> 4. Residual Matrix</a></li>
  </ul></li>
  <li><a href="#aerospace-interpretation" id="toc-aerospace-interpretation" class="nav-link" data-scroll-target="#aerospace-interpretation"><span class="header-section-number">13.4</span> Aerospace Interpretation</a></li>
  </ul></li>
  <li><a href="#when-direct-solution-fails" id="toc-when-direct-solution-fails" class="nav-link" data-scroll-target="#when-direct-solution-fails"><span class="header-section-number">14</span> When Direct Solution Fails</a>
  <ul class="collapse">
  <li><a href="#problem-1-singular-matrix-non-invertibility" id="toc-problem-1-singular-matrix-non-invertibility" class="nav-link" data-scroll-target="#problem-1-singular-matrix-non-invertibility"><span class="header-section-number">14.1</span> Problem 1: Singular Matrix (Non-Invertibility)</a></li>
  <li><a href="#problem-2-computational-cost" id="toc-problem-2-computational-cost" class="nav-link" data-scroll-target="#problem-2-computational-cost"><span class="header-section-number">14.2</span> Problem 2: Computational Cost</a></li>
  <li><a href="#problem-3-numerical-stability" id="toc-problem-3-numerical-stability" class="nav-link" data-scroll-target="#problem-3-numerical-stability"><span class="header-section-number">14.3</span> Problem 3: Numerical Stability</a></li>
  <li><a href="#solutions" id="toc-solutions" class="nav-link" data-scroll-target="#solutions"><span class="header-section-number">14.4</span> Solutions</a>
  <ul class="collapse">
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="header-section-number">14.4.1</span> 1. Regularization</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent"><span class="header-section-number">14.4.2</span> 2. Gradient Descent</a></li>
  <li><a href="#qr-decomposition" id="toc-qr-decomposition" class="nav-link" data-scroll-target="#qr-decomposition"><span class="header-section-number">14.4.3</span> 3. QR Decomposition</a></li>
  <li><a href="#singular-value-decomposition-svd" id="toc-singular-value-decomposition-svd" class="nav-link" data-scroll-target="#singular-value-decomposition-svd"><span class="header-section-number">14.4.4</span> 4. Singular Value Decomposition (SVD)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#gradient-descent-iterative-approach" id="toc-gradient-descent-iterative-approach" class="nav-link" data-scroll-target="#gradient-descent-iterative-approach"><span class="header-section-number">15</span> Gradient Descent: Iterative Approach</a>
  <ul class="collapse">
  <li><a href="#the-algorithm" id="toc-the-algorithm" class="nav-link" data-scroll-target="#the-algorithm"><span class="header-section-number">15.1</span> The Algorithm</a>
  <ul class="collapse">
  <li><a href="#steps" id="toc-steps" class="nav-link" data-scroll-target="#steps"><span class="header-section-number">15.1.1</span> Steps</a></li>
  </ul></li>
  <li><a href="#computing-the-gradient" id="toc-computing-the-gradient" class="nav-link" data-scroll-target="#computing-the-gradient"><span class="header-section-number">15.2</span> Computing the Gradient</a></li>
  <li><a href="#convergence" id="toc-convergence" class="nav-link" data-scroll-target="#convergence"><span class="header-section-number">15.3</span> Convergence</a></li>
  </ul></li>
  <li><a href="#gradient-descent-variants" id="toc-gradient-descent-variants" class="nav-link" data-scroll-target="#gradient-descent-variants"><span class="header-section-number">16</span> Gradient Descent Variants</a>
  <ul class="collapse">
  <li><a href="#batch-gradient-descent" id="toc-batch-gradient-descent" class="nav-link" data-scroll-target="#batch-gradient-descent"><span class="header-section-number">16.1</span> Batch Gradient Descent</a></li>
  <li><a href="#stochastic-gradient-descent-sgd" id="toc-stochastic-gradient-descent-sgd" class="nav-link" data-scroll-target="#stochastic-gradient-descent-sgd"><span class="header-section-number">16.2</span> Stochastic Gradient Descent (SGD)</a></li>
  <li><a href="#mini-batch-gradient-descent" id="toc-mini-batch-gradient-descent" class="nav-link" data-scroll-target="#mini-batch-gradient-descent"><span class="header-section-number">16.3</span> Mini-Batch Gradient Descent</a></li>
  <li><a href="#comparison-summary" id="toc-comparison-summary" class="nav-link" data-scroll-target="#comparison-summary"><span class="header-section-number">16.4</span> Comparison Summary</a></li>
  </ul></li>
  <li><a href="#learning-rate-selection" id="toc-learning-rate-selection" class="nav-link" data-scroll-target="#learning-rate-selection"><span class="header-section-number">17</span> Learning Rate Selection</a>
  <ul class="collapse">
  <li><a href="#the-role-of-learning-rate" id="toc-the-role-of-learning-rate" class="nav-link" data-scroll-target="#the-role-of-learning-rate"><span class="header-section-number">17.1</span> The Role of Learning Rate</a></li>
  <li><a href="#too-small-learning-rate-eta-ll-1" id="toc-too-small-learning-rate-eta-ll-1" class="nav-link" data-scroll-target="#too-small-learning-rate-eta-ll-1"><span class="header-section-number">17.2</span> Too Small Learning Rate (<span class="math inline">\(\eta \ll 1\)</span>)</a></li>
  <li><a href="#too-large-learning-rate-eta-gg-1" id="toc-too-large-learning-rate-eta-gg-1" class="nav-link" data-scroll-target="#too-large-learning-rate-eta-gg-1"><span class="header-section-number">17.3</span> Too Large Learning Rate (<span class="math inline">\(\eta \gg 1\)</span>)</a></li>
  <li><a href="#finding-the-right-learning-rate" id="toc-finding-the-right-learning-rate" class="nav-link" data-scroll-target="#finding-the-right-learning-rate"><span class="header-section-number">17.4</span> Finding the Right Learning Rate</a></li>
  <li><a href="#adaptive-learning-rates" id="toc-adaptive-learning-rates" class="nav-link" data-scroll-target="#adaptive-learning-rates"><span class="header-section-number">17.5</span> Adaptive Learning Rates</a>
  <ul class="collapse">
  <li><a href="#learning-rate-decay" id="toc-learning-rate-decay" class="nav-link" data-scroll-target="#learning-rate-decay"><span class="header-section-number">17.5.1</span> 1. Learning Rate Decay</a></li>
  <li><a href="#momentum" id="toc-momentum" class="nav-link" data-scroll-target="#momentum"><span class="header-section-number">17.5.2</span> 2. Momentum</a></li>
  <li><a href="#adam-adaptive-moment-estimation" id="toc-adam-adaptive-moment-estimation" class="nav-link" data-scroll-target="#adam-adaptive-moment-estimation"><span class="header-section-number">17.5.3</span> 3. Adam (Adaptive Moment Estimation)</a></li>
  <li><a href="#line-search" id="toc-line-search" class="nav-link" data-scroll-target="#line-search"><span class="header-section-number">17.5.4</span> 4. Line Search</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#statistical-properties-assumptions" id="toc-statistical-properties-assumptions" class="nav-link" data-scroll-target="#statistical-properties-assumptions"><span class="header-section-number">18</span> Statistical Properties: Assumptions</a>
  <ul class="collapse">
  <li><a href="#classical-linear-regression-assumptions" id="toc-classical-linear-regression-assumptions" class="nav-link" data-scroll-target="#classical-linear-regression-assumptions"><span class="header-section-number">18.1</span> Classical Linear Regression Assumptions</a>
  <ul class="collapse">
  <li><a href="#linearity" id="toc-linearity" class="nav-link" data-scroll-target="#linearity"><span class="header-section-number">18.1.1</span> 1. Linearity</a></li>
  <li><a href="#independence" id="toc-independence" class="nav-link" data-scroll-target="#independence"><span class="header-section-number">18.1.2</span> 2. Independence</a></li>
  <li><a href="#homoscedasticity-constant-variance" id="toc-homoscedasticity-constant-variance" class="nav-link" data-scroll-target="#homoscedasticity-constant-variance"><span class="header-section-number">18.1.3</span> 3. Homoscedasticity (Constant Variance)</a></li>
  <li><a href="#normality" id="toc-normality" class="nav-link" data-scroll-target="#normality"><span class="header-section-number">18.1.4</span> 4. Normality</a></li>
  <li><a href="#no-perfect-multicollinearity" id="toc-no-perfect-multicollinearity" class="nav-link" data-scroll-target="#no-perfect-multicollinearity"><span class="header-section-number">18.1.5</span> 5. No Perfect Multicollinearity</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#gauss-markov-theorem-implications-for-practice" id="toc-gauss-markov-theorem-implications-for-practice" class="nav-link" data-scroll-target="#gauss-markov-theorem-implications-for-practice"><span class="header-section-number">19</span> Gauss-Markov Theorem: Implications for Practice</a>
  <ul class="collapse">
  <li><a href="#statement-of-the-theorem" id="toc-statement-of-the-theorem" class="nav-link" data-scroll-target="#statement-of-the-theorem"><span class="header-section-number">19.1</span> Statement of the Theorem</a></li>
  <li><a href="#what-does-blue-mean" id="toc-what-does-blue-mean" class="nav-link" data-scroll-target="#what-does-blue-mean"><span class="header-section-number">19.2</span> What Does BLUE Mean?</a>
  <ul class="collapse">
  <li><a href="#best" id="toc-best" class="nav-link" data-scroll-target="#best"><span class="header-section-number">19.2.1</span> Best</a></li>
  <li><a href="#linear" id="toc-linear" class="nav-link" data-scroll-target="#linear"><span class="header-section-number">19.2.2</span> Linear</a></li>
  <li><a href="#unbiased" id="toc-unbiased" class="nav-link" data-scroll-target="#unbiased"><span class="header-section-number">19.2.3</span> Unbiased</a></li>
  </ul></li>
  <li><a href="#aerospace-context" id="toc-aerospace-context" class="nav-link" data-scroll-target="#aerospace-context"><span class="header-section-number">19.3</span> Aerospace Context</a></li>
  <li><a href="#when-blue-doesnt-apply" id="toc-when-blue-doesnt-apply" class="nav-link" data-scroll-target="#when-blue-doesnt-apply"><span class="header-section-number">19.4</span> When BLUE Doesn’t Apply</a></li>
  </ul></li>
  <li><a href="#statistical-properties-distribution" id="toc-statistical-properties-distribution" class="nav-link" data-scroll-target="#statistical-properties-distribution"><span class="header-section-number">20</span> Statistical Properties: Distribution</a>
  <ul class="collapse">
  <li><a href="#distribution-of-the-ols-estimator" id="toc-distribution-of-the-ols-estimator" class="nav-link" data-scroll-target="#distribution-of-the-ols-estimator"><span class="header-section-number">20.1</span> Distribution of the OLS Estimator</a></li>
  <li><a href="#unpacking-this-result" id="toc-unpacking-this-result" class="nav-link" data-scroll-target="#unpacking-this-result"><span class="header-section-number">20.2</span> Unpacking This Result</a>
  <ul class="collapse">
  <li><a href="#mean-expected-value" id="toc-mean-expected-value" class="nav-link" data-scroll-target="#mean-expected-value"><span class="header-section-number">20.2.1</span> Mean (Expected Value)</a></li>
  <li><a href="#covariance-matrix" id="toc-covariance-matrix" class="nav-link" data-scroll-target="#covariance-matrix"><span class="header-section-number">20.2.2</span> Covariance Matrix</a></li>
  </ul></li>
  <li><a href="#practical-implications" id="toc-practical-implications" class="nav-link" data-scroll-target="#practical-implications"><span class="header-section-number">20.3</span> Practical Implications</a></li>
  <li><a href="#what-if-normality-doesnt-hold" id="toc-what-if-normality-doesnt-hold" class="nav-link" data-scroll-target="#what-if-normality-doesnt-hold"><span class="header-section-number">20.4</span> What If Normality Doesn’t Hold?</a></li>
  </ul></li>
  <li><a href="#estimating-the-noise-variance" id="toc-estimating-the-noise-variance" class="nav-link" data-scroll-target="#estimating-the-noise-variance"><span class="header-section-number">21</span> Estimating the Noise Variance</a>
  <ul class="collapse">
  <li><a href="#residual-variance-estimator" id="toc-residual-variance-estimator" class="nav-link" data-scroll-target="#residual-variance-estimator"><span class="header-section-number">21.1</span> Residual Variance Estimator</a></li>
  <li><a href="#why-divide-by-n---d---1" id="toc-why-divide-by-n---d---1" class="nav-link" data-scroll-target="#why-divide-by-n---d---1"><span class="header-section-number">21.2</span> Why Divide by <span class="math inline">\(n - d - 1\)</span>?</a>
  <ul class="collapse">
  <li><a href="#degrees-of-freedom" id="toc-degrees-of-freedom" class="nav-link" data-scroll-target="#degrees-of-freedom"><span class="header-section-number">21.2.1</span> Degrees of Freedom</a></li>
  <li><a href="#why-not-divide-by-n" id="toc-why-not-divide-by-n" class="nav-link" data-scroll-target="#why-not-divide-by-n"><span class="header-section-number">21.2.2</span> Why Not Divide by <span class="math inline">\(n\)</span>?</a></li>
  </ul></li>
  <li><a href="#aerospace-example-1" id="toc-aerospace-example-1" class="nav-link" data-scroll-target="#aerospace-example-1"><span class="header-section-number">21.3</span> Aerospace Example</a></li>
  <li><a href="#relationship-to-model-fit" id="toc-relationship-to-model-fit" class="nav-link" data-scroll-target="#relationship-to-model-fit"><span class="header-section-number">21.4</span> Relationship to Model Fit</a></li>
  </ul></li>
  <li><a href="#confidence-intervals-for-coefficients" id="toc-confidence-intervals-for-coefficients" class="nav-link" data-scroll-target="#confidence-intervals-for-coefficients"><span class="header-section-number">22</span> Confidence Intervals for Coefficients</a>
  <ul class="collapse">
  <li><a href="#understanding-coefficient-uncertainty" id="toc-understanding-coefficient-uncertainty" class="nav-link" data-scroll-target="#understanding-coefficient-uncertainty"><span class="header-section-number">22.1</span> Understanding Coefficient Uncertainty</a></li>
  <li><a href="#standard-error" id="toc-standard-error" class="nav-link" data-scroll-target="#standard-error"><span class="header-section-number">22.2</span> Standard Error</a></li>
  <li><a href="#confidence-interval-formula" id="toc-confidence-interval-formula" class="nav-link" data-scroll-target="#confidence-interval-formula"><span class="header-section-number">22.3</span> Confidence Interval Formula</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation"><span class="header-section-number">22.4</span> Interpretation</a></li>
  <li><a href="#aerospace-example-2" id="toc-aerospace-example-2" class="nav-link" data-scroll-target="#aerospace-example-2"><span class="header-section-number">22.5</span> Aerospace Example</a></li>
  </ul></li>
  <li><a href="#hypothesis-testing-for-individual-coefficients" id="toc-hypothesis-testing-for-individual-coefficients" class="nav-link" data-scroll-target="#hypothesis-testing-for-individual-coefficients"><span class="header-section-number">23</span> Hypothesis Testing for Individual Coefficients</a>
  <ul class="collapse">
  <li><a href="#the-central-question" id="toc-the-central-question" class="nav-link" data-scroll-target="#the-central-question"><span class="header-section-number">23.1</span> The Central Question</a></li>
  <li><a href="#setting-up-the-test" id="toc-setting-up-the-test" class="nav-link" data-scroll-target="#setting-up-the-test"><span class="header-section-number">23.2</span> Setting Up the Test</a>
  <ul class="collapse">
  <li><a href="#hypotheses" id="toc-hypotheses" class="nav-link" data-scroll-target="#hypotheses"><span class="header-section-number">23.2.1</span> Hypotheses</a></li>
  </ul></li>
  <li><a href="#the-test-statistic" id="toc-the-test-statistic" class="nav-link" data-scroll-target="#the-test-statistic"><span class="header-section-number">23.3</span> The Test Statistic</a></li>
  <li><a href="#distribution-under-the-null" id="toc-distribution-under-the-null" class="nav-link" data-scroll-target="#distribution-under-the-null"><span class="header-section-number">23.4</span> Distribution Under the Null</a></li>
  <li><a href="#making-the-decision" id="toc-making-the-decision" class="nav-link" data-scroll-target="#making-the-decision"><span class="header-section-number">23.5</span> Making the Decision</a>
  <ul class="collapse">
  <li><a href="#approach-1-p-value" id="toc-approach-1-p-value" class="nav-link" data-scroll-target="#approach-1-p-value"><span class="header-section-number">23.5.1</span> Approach 1: P-value</a></li>
  <li><a href="#approach-2-critical-value" id="toc-approach-2-critical-value" class="nav-link" data-scroll-target="#approach-2-critical-value"><span class="header-section-number">23.5.2</span> Approach 2: Critical Value</a></li>
  </ul></li>
  <li><a href="#aerospace-example-3" id="toc-aerospace-example-3" class="nav-link" data-scroll-target="#aerospace-example-3"><span class="header-section-number">23.6</span> Aerospace Example</a></li>
  <li><a href="#important-caveats" id="toc-important-caveats" class="nav-link" data-scroll-target="#important-caveats"><span class="header-section-number">23.7</span> Important Caveats</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">24</span> Summary</a>
  <ul class="collapse">
  <li><a href="#mathematical-foundations" id="toc-mathematical-foundations" class="nav-link" data-scroll-target="#mathematical-foundations"><span class="header-section-number">24.1</span> Mathematical Foundations</a></li>
  <li><a href="#two-solution-approaches" id="toc-two-solution-approaches" class="nav-link" data-scroll-target="#two-solution-approaches"><span class="header-section-number">24.2</span> Two Solution Approaches</a></li>
  <li><a href="#geometric-insight" id="toc-geometric-insight" class="nav-link" data-scroll-target="#geometric-insight"><span class="header-section-number">24.3</span> Geometric Insight</a></li>
  <li><a href="#statistical-properties" id="toc-statistical-properties" class="nav-link" data-scroll-target="#statistical-properties"><span class="header-section-number">24.4</span> Statistical Properties</a></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations"><span class="header-section-number">24.5</span> Practical Considerations</a></li>
  <li><a href="#aerospace-applications" id="toc-aerospace-applications" class="nav-link" data-scroll-target="#aerospace-applications"><span class="header-section-number">24.6</span> Aerospace Applications</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Linear Regression: Reading Material</h1>
<p class="subtitle lead">AERO 689: Introduction to Machine Learning for Aerospace Engineers</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Dr.&nbsp;Raktim Bhattacharya </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Texas A&amp;M University - Aerospace Engineering
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Linear regression is one of the most fundamental and widely used techniques in machine learning and statistical modeling. In aerospace engineering, it plays a crucial role in predicting aircraft performance, analyzing wind tunnel data, and optimizing flight operations. This reading material provides detailed explanations of the concepts presented in the lecture slides, with a focus on both mathematical rigor and practical aerospace applications.</p>
<section id="learning-objectives" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">1.1</span> Learning Objectives</h2>
<p>By the end of this module, you should be able to:</p>
<ol type="1">
<li><p><strong>Apply linear regression to aerospace performance prediction</strong>: Understand how to use linear regression to model relationships between flight parameters (e.g., angle of attack, Mach number) and performance metrics (e.g., drag coefficient, fuel consumption).</p></li>
<li><p><strong>Understand least squares method and gradient descent</strong>: Master both the analytical (closed-form) and iterative (gradient descent) approaches to solving linear regression problems.</p></li>
<li><p><strong>Implement drag coefficient prediction from wind tunnel data</strong>: Learn to process real experimental data and build predictive models that account for measurement noise and physical constraints.</p></li>
<li><p><strong>Validate models using aerospace-specific metrics</strong>: Understand how to assess model quality using appropriate statistical measures and ensure predictions meet safety and certification requirements.</p></li>
</ol>
</section>
</section>
<section id="motivation-the-fuel-crisis-challenge" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Motivation: The Fuel Crisis Challenge</h1>
<section id="the-100-million-question" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="the-100-million-question"><span class="header-section-number">2.1</span> The $100 Million Question</h2>
<p>Consider a real-world scenario that aerospace engineers face daily: an airline operates a fleet of 200 aircraft. The fuel cost alone exceeds $50 million annually per aircraft type. With such enormous operational costs, even small improvements in fuel efficiency can translate to massive savings.</p>
<p><strong>The Challenge</strong>: Airlines need to accurately predict fuel consumption for flight planning. Currently, most operations rely on simplified performance charts that were developed under idealized conditions. These charts provide conservative estimates but may not capture the full complexity of real-world flight operations.</p>
<p><strong>The Machine Learning Opportunity</strong>: By using actual flight data collected from thousands of flights, we can build precise models that account for:</p>
<ul>
<li>Actual weather conditions encountered</li>
<li>Real payload and weight variations</li>
<li>Engine performance degradation over time</li>
<li>Pilot technique variations</li>
<li>Air traffic control routing constraints</li>
</ul>
</section>
<section id="real-impact" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="real-impact"><span class="header-section-number">2.2</span> Real Impact</h2>
<p>The potential benefits of improved fuel consumption models are substantial:</p>
<ul>
<li><strong>1% fuel savings</strong> = Over $100 million annually across the industry</li>
<li><strong>Better range predictions</strong> = More efficient route planning and optimization</li>
<li><strong>Accurate payload calculations</strong> = Improved safety margins while maximizing revenue cargo capacity</li>
</ul>
<p><strong>Discussion Question</strong>: What factors do you think affect aircraft fuel consumption? Consider environmental factors (weather, altitude, temperature), operational factors (weight, speed, routing), and mechanical factors (engine condition, aerodynamic efficiency).</p>
</section>
</section>
<section id="from-wind-tunnel-to-flight-the-data-challenge" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> From Wind Tunnel to Flight: The Data Challenge</h1>
<section id="traditional-approach-empirical-models" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="traditional-approach-empirical-models"><span class="header-section-number">3.1</span> Traditional Approach: Empirical Models</h2>
<p>Aerospace engineers have long used empirical models to characterize aircraft performance. One classic example is the <strong>parabolic drag polar</strong>:</p>
<p><span class="math display">\[
C_D = C_{D_0} + KC_L^2
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(C_D\)</span> is the drag coefficient</li>
<li><span class="math inline">\(C_{D_0}\)</span> is the zero-lift drag coefficient (parasitic drag)</li>
<li><span class="math inline">\(K\)</span> is the induced drag factor</li>
<li><span class="math inline">\(C_L\)</span> is the lift coefficient</li>
</ul>
<p><strong>The Problem</strong>: This model assumes perfectly controlled conditions—smooth flow, steady state, clean configuration. In reality:</p>
<ul>
<li>Real flights encounter turbulence, wind shear, and varying atmospheric conditions</li>
<li>Aircraft weight changes continuously as fuel is consumed</li>
<li>Engines degrade over time, affecting performance</li>
<li>Manufacturing tolerances mean each aircraft is slightly different</li>
</ul>
<p><strong>The Solution</strong>: Machine learning allows us to learn patterns directly from operational data, capturing complexities that simplified analytical models may miss.</p>
</section>
<section id="available-data-sources" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="available-data-sources"><span class="header-section-number">3.2</span> Available Data Sources</h2>
<p>Aerospace engineers can draw from three main sources of data:</p>
<section id="wind-tunnel-data" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="wind-tunnel-data"><span class="header-section-number">3.2.1</span> 1. Wind Tunnel Data</h3>
<p><strong>Characteristics</strong>: - Highly controlled environment - Precise measurements - Limited range of conditions - Expensive to collect - May not capture full-scale Reynolds number effects</p>
<p><strong>Best for</strong>: Understanding fundamental aerodynamic behavior, validating CFD simulations</p>
</section>
<section id="flight-test-data" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="flight-test-data"><span class="header-section-number">3.2.2</span> 2. Flight Test Data</h3>
<p><strong>Characteristics</strong>: - Real flight conditions - Instrumented aircraft - Very expensive to collect (dedicated test aircraft, crew, facilities) - Limited sample size - High quality, well-documented</p>
<p><strong>Best for</strong>: Aircraft certification, validating performance predictions, boundary exploration</p>
</section>
<section id="operational-data" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="operational-data"><span class="header-section-number">3.2.3</span> 3. Operational Data</h3>
<p><strong>Characteristics</strong>: - Massive scale (thousands or millions of flights) - Representative of actual operations - Noisy (sensor errors, environmental variations) - May lack detailed instrumentation - Continuously collected</p>
<p><strong>Best for</strong>: Statistical modeling, fleet-wide trends, operational optimization</p>
</section>
</section>
<section id="the-linear-regression-framework" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="the-linear-regression-framework"><span class="header-section-number">3.3</span> The Linear Regression Framework</h2>
<p>For this module, we’ll focus on a specific problem: predicting the drag coefficient from flight parameters.</p>
<p><strong>Goal</strong>: Predict <span class="math inline">\(C_D\)</span> (drag coefficient) accurately</p>
<p><strong>Input Features</strong>: - <span class="math inline">\(\alpha\)</span> (angle of attack) - <span class="math inline">\(M\)</span> (Mach number) - <span class="math inline">\(Re\)</span> (Reynolds number) - Potentially interaction terms and polynomial features</p>
<p><strong>Output</strong>: Drag coefficient value for performance calculations</p>
<p>Why is this important? Accurate drag prediction is essential for: - Fuel consumption estimation - Range calculations - Climb performance - Flight envelope determination - Control system design</p>
</section>
</section>
<section id="mathematical-foundation-the-linear-model" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Mathematical Foundation: The Linear Model</h1>
<section id="general-form" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="general-form"><span class="header-section-number">4.1</span> General Form</h2>
<p>Linear regression models the relationship between input features and a continuous output variable. For a dataset with <span class="math inline">\(n\)</span> samples and <span class="math inline">\(d\)</span> features, the model is:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_d x_{id} + \epsilon_i
\]</span></p>
<p><strong>Notation Explained</strong>:</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span>: The response variable (dependent variable) for sample <span class="math inline">\(i\)</span>. In aerospace: this could be drag coefficient, fuel flow rate, or any measurable output.</p></li>
<li><p><span class="math inline">\(x_{ij}\)</span>: The <span class="math inline">\(j\)</span>-th feature (independent variable) of the <span class="math inline">\(i\)</span>-th sample. Examples: Mach number, angle of attack, altitude, etc.</p></li>
<li><p><span class="math inline">\(\beta_j\)</span>: Regression coefficients (parameters) that we need to learn from data. These quantify how each feature affects the response.</p></li>
<li><p><span class="math inline">\(\beta_0\)</span>: The intercept term. This represents the baseline value when all features are zero.</p></li>
<li><p><span class="math inline">\(\epsilon_i\)</span>: The error term (residual). This captures:</p>
<ul>
<li>Measurement noise</li>
<li>Unmodeled physics</li>
<li>Random variations</li>
<li>Model approximation errors</li>
</ul></li>
</ul>
</section>
<section id="vector-notation" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="vector-notation"><span class="header-section-number">4.2</span> Vector Notation</h2>
<p>To work with all samples simultaneously, we use matrix-vector notation:</p>
<p><span class="math display">\[
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}^\ast + \boldsymbol{\epsilon}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^n\)</span>: Vector of all <span class="math inline">\(n\)</span> response values</li>
<li><span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{n \times (d+1)}\)</span>: Design matrix (also called feature matrix)</li>
<li><span class="math inline">\(\boldsymbol{\beta}^\ast \in \mathbb{R}^{d+1}\)</span>: Vector of true parameters (including intercept)</li>
<li><span class="math inline">\(\boldsymbol{\epsilon} \in \mathbb{R}^n\)</span>: Vector of all error terms</li>
</ul>
<p>The design matrix <span class="math inline">\(\boldsymbol{X}\)</span> is augmented with a column of ones to account for the intercept term.</p>
</section>
</section>
<section id="matrix-formulation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Matrix Formulation</h1>
<section id="design-matrix-structure" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="design-matrix-structure"><span class="header-section-number">5.1</span> Design Matrix Structure</h2>
<p>The design matrix organizes all our data into a structured format:</p>
<p><span class="math display">\[
\boldsymbol{X} = \begin{bmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} \\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nd}
\end{bmatrix}
\]</span></p>
<p><strong>Understanding the structure</strong>:</p>
<ul>
<li>Each <strong>row</strong> represents one data sample (one observation, one flight, one wind tunnel measurement)</li>
<li>Each <strong>column</strong> (except the first) represents one feature across all samples</li>
<li>The <strong>first column</strong> of all ones corresponds to the intercept term <span class="math inline">\(\beta_0\)</span></li>
<li>Dimensions: <span class="math inline">\(n\)</span> rows × <span class="math inline">\((d+1)\)</span> columns</li>
</ul>
<p>The parameter vector and response vector are:</p>
<p><span class="math display">\[
\boldsymbol{\beta} = \begin{bmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_d
\end{bmatrix}, \quad
\boldsymbol{y} = \begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{bmatrix}
\]</span></p>
</section>
<section id="aerospace-example-drag-prediction" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="aerospace-example-drag-prediction"><span class="header-section-number">5.2</span> Aerospace Example: Drag Prediction</h2>
<p>Let’s make this concrete with a drag coefficient prediction problem.</p>
<p><strong>Scenario</strong>: We have wind tunnel data measuring drag coefficient at various angles of attack and Mach numbers. We want to build a model that includes: - Linear terms: <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(M\)</span>, <span class="math inline">\(Re\)</span> - Quadratic terms: <span class="math inline">\(\alpha^2\)</span>, <span class="math inline">\(M^2\)</span> - Interaction terms: <span class="math inline">\(\alpha M\)</span>, <span class="math inline">\(\alpha Re\)</span>, etc.</p>
<p><strong>Design Matrix</strong>: <span class="math display">\[
\boldsymbol{X} = \begin{bmatrix}
1 &amp; \alpha_1 &amp; M_1 &amp; \text{Re}_1 &amp; \alpha_1^2 &amp; M_1^2 &amp; \alpha_1 M_1 &amp; \cdots \\
1 &amp; \alpha_2 &amp; M_2 &amp; \text{Re}_2 &amp; \alpha_2^2 &amp; M_2^2 &amp; \alpha_2 M_2 &amp; \cdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{bmatrix}
\]</span></p>
<p><strong>Parameter Vector</strong>: <span class="math display">\[
\boldsymbol{\beta} = \begin{bmatrix}
C_{D_0} \\ k_\alpha \\ k_M \\ k_{\text{Re}} \\ k_{\alpha^2} \\ k_{M^2} \\ k_{\alpha M} \\ \cdots
\end{bmatrix}^T
\]</span></p>
<p><strong>Target Vector</strong>: <span class="math display">\[
\boldsymbol{y} = \begin{bmatrix}
C_{D_1} \\ C_{D_2} \\ \cdots \\ C_{D_n}
\end{bmatrix}^T
\]</span></p>
<p>This formulation allows us to capture complex aerodynamic behavior while maintaining the linear regression framework.</p>
</section>
</section>
<section id="key-insight-what-linear-means" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Key Insight: What “Linear” Means</h1>
<section id="linear-in-parameters-not-features" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="linear-in-parameters-not-features"><span class="header-section-number">6.1</span> Linear in Parameters, Not Features</h2>
<p>A common source of confusion: “linear regression” refers to linearity <strong>in the parameters</strong> <span class="math inline">\(\boldsymbol{\beta}\)</span>, <strong>not</strong> in the input features <span class="math inline">\(\boldsymbol{x}\)</span>.</p>
<p><strong>The General Form</strong>: <span class="math display">\[
y = \beta_0 \phi_0(\boldsymbol{x}) + \beta_1 \phi_1(\boldsymbol{x}) + \cdots + \beta_d \phi_d(\boldsymbol{x})
\]</span></p>
<p>where <span class="math inline">\(\phi_j(\boldsymbol{x})\)</span> are <strong>basis functions</strong> that can be: - Linear: <span class="math inline">\(\phi_1(x) = x\)</span> - Polynomial: <span class="math inline">\(\phi_2(x) = x^2\)</span>, <span class="math inline">\(\phi_3(x) = x^3\)</span> - Trigonometric: <span class="math inline">\(\phi_4(x) = \sin(x)\)</span>, <span class="math inline">\(\phi_5(x) = \cos(x)\)</span> - Exponential: <span class="math inline">\(\phi_6(x) = e^x\)</span> - Interactions: <span class="math inline">\(\phi_7(x_1, x_2) = x_1 x_2\)</span> - Any other nonlinear transformation</p>
<p><strong>Key Properties</strong>:</p>
<ol type="1">
<li><strong>Basis functions</strong> <span class="math inline">\(\phi_j(\boldsymbol{x})\)</span> are <strong>fixed</strong> (you choose them before fitting)</li>
<li><strong>Coefficients</strong> <span class="math inline">\(\beta_j\)</span> are <strong>what we solve for</strong> (learned from data)</li>
<li>The model is <strong>linear</strong> in <span class="math inline">\(\beta_j\)</span> because each <span class="math inline">\(\beta_j\)</span> appears with power 1 and no products of different <span class="math inline">\(\beta_j\)</span> terms</li>
<li>The model can be <strong>nonlinear</strong> in <span class="math inline">\(\boldsymbol{x}\)</span> because the basis functions can transform inputs arbitrarily</li>
</ol>
</section>
<section id="why-linearity-in-parameters-matters" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="why-linearity-in-parameters-matters"><span class="header-section-number">6.2</span> Why Linearity in Parameters Matters</h2>
<p><strong>Mathematical Benefit</strong>: Linearity in parameters means:</p>
<ul>
<li>The optimization problem is <strong>convex</strong> (has exactly one global minimum, no local minima)</li>
<li>A <strong>closed-form solution</strong> exists (we can write down the exact answer)</li>
<li>The solution is <strong>unique</strong> (if <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> is invertible)</li>
<li>We can use <strong>efficient linear algebra</strong> algorithms</li>
</ul>
<p><strong>Practical Benefit</strong>: We can model complex, nonlinear physical phenomena while maintaining: - Computational efficiency - Guaranteed convergence - Interpretable coefficients - Statistical properties (confidence intervals, hypothesis tests)</p>
</section>
<section id="aerospace-example" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="aerospace-example"><span class="header-section-number">6.3</span> Aerospace Example</h2>
<p>Consider modeling the drag coefficient with this equation:</p>
<p><span class="math display">\[
C_D = \beta_0 + \beta_1\alpha + \beta_2\alpha^2 + \beta_3 M^2 + \beta_4(\alpha M)
\]</span></p>
<p><strong>Analysis</strong>:</p>
<ul>
<li><p><strong>Nonlinear in physical variables</strong>: The relationship between <span class="math inline">\(C_D\)</span> and <span class="math inline">\((\alpha, M)\)</span> is nonlinear due to the <span class="math inline">\(\alpha^2\)</span>, <span class="math inline">\(M^2\)</span>, and <span class="math inline">\(\alpha M\)</span> terms. This is a parabolic surface in the <span class="math inline">\((\alpha, M)\)</span> space.</p></li>
<li><p><strong>Linear in parameters</strong>: The equation is a weighted sum of basis functions: <span class="math display">\[C_D = \beta_0 \cdot 1 + \beta_1 \cdot \alpha + \beta_2 \cdot \alpha^2 + \beta_3 \cdot M^2 + \beta_4 \cdot (\alpha M)\]</span></p></li>
<li><p><strong>Effect of doubling <span class="math inline">\(\beta_2\)</span></strong>: If we double <span class="math inline">\(\beta_2\)</span>, the contribution of the <span class="math inline">\(\alpha^2\)</span> term exactly doubles. This linear relationship in the parameters is what makes the problem tractable.</p></li>
</ul>
<p><strong>Matrix Representation</strong>: <span class="math display">\[
\boldsymbol{X} = \begin{bmatrix}
1 &amp; \alpha_1 &amp; \alpha_1^2 &amp; M_1^2 &amp; \alpha_1 M_1 \\
1 &amp; \alpha_2 &amp; \alpha_2^2 &amp; M_2^2 &amp; \alpha_2 M_2 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots
\end{bmatrix}
\]</span></p>
<p>Each row processes one data point through all basis functions, creating a standard linear regression problem.</p>
</section>
</section>
<section id="real-data-noisy-measurements" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Real Data: Noisy Measurements</h1>
<section id="understanding-measurement-noise" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="understanding-measurement-noise"><span class="header-section-number">7.1</span> Understanding Measurement Noise</h2>
<p>In practice, all experimental data contains measurement errors. For aerospace applications, these errors arise from:</p>
<ol type="1">
<li><strong>Sensor limitations</strong>: Finite precision, drift, calibration errors</li>
<li><strong>Environmental factors</strong>: Temperature variations, atmospheric turbulence</li>
<li><strong>Flow unsteadiness</strong>: Turbulent fluctuations, vortex shedding</li>
<li><strong>Model simplification</strong>: Real physics is more complex than our model</li>
<li><strong>Human factors</strong>: Data recording errors, experimental setup variations</li>
</ol>
</section>
<section id="characteristics-of-real-wind-tunnel-data" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="characteristics-of-real-wind-tunnel-data"><span class="header-section-number">7.2</span> Characteristics of Real Wind Tunnel Data</h2>
<p>When measuring drag coefficients in a wind tunnel, we typically observe:</p>
<ul>
<li><strong>Data scatter</strong> around the true relationship</li>
<li><strong>Heteroscedastic noise</strong>: Measurement uncertainty often increases with angle of attack (larger forces → larger absolute errors)</li>
<li><strong>Systematic biases</strong>: Wall effects, support interference</li>
<li><strong>Outliers</strong>: Occasionally anomalous measurements due to flow separation or experimental issues</li>
</ul>
</section>
<section id="why-the-error-term-epsilon_i-is-essential" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="why-the-error-term-epsilon_i-is-essential"><span class="header-section-number">7.3</span> Why the Error Term <span class="math inline">\(\epsilon_i\)</span> is Essential</h2>
<p>The error term in our model <span class="math inline">\(y_i = \boldsymbol{x}_i^T\boldsymbol{\beta}^\ast + \epsilon_i\)</span> is not just a mathematical convenience—it’s a fundamental recognition that:</p>
<ol type="1">
<li><strong>No model is perfect</strong>: Even the best physical model cannot capture every detail</li>
<li><strong>Measurements are imperfect</strong>: Sensors have inherent limitations</li>
<li><strong>Random variations exist</strong>: Physical processes have inherent stochasticity</li>
<li><strong>We seek expected behavior</strong>: Our goal is to find the average relationship, not fit every noise fluctuation</li>
</ol>
<p><strong>Important</strong>: We want our model to capture the <strong>signal</strong> (true underlying relationship) without overfitting to the <strong>noise</strong> (random fluctuations). This is the essence of good statistical modeling.</p>
</section>
</section>
<section id="the-optimization-problem" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> The Optimization Problem</h1>
<section id="objective-minimize-squared-error" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="objective-minimize-squared-error"><span class="header-section-number">8.1</span> Objective: Minimize Squared Error</h2>
<p>Given data <span class="math inline">\(\{(\boldsymbol{x}_i, y_i)\}_{i=1}^n\)</span>, we want to find parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> that make our predictions <span class="math inline">\(\hat{y}_i = \boldsymbol{x}_i^T\boldsymbol{\beta}\)</span> as close as possible to the observed values <span class="math inline">\(y_i\)</span>.</p>
<p><strong>Residual for sample <span class="math inline">\(i\)</span></strong>: <span class="math display">\[
r_i = y_i - \hat{y}_i = y_i - \boldsymbol{x}_i^T\boldsymbol{\beta}
\]</span></p>
<p><strong>Residual Sum of Squares (RSS)</strong>: <span class="math display">\[
\text{RSS}(\boldsymbol{\beta}) = \sum_{i=1}^n r_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - \boldsymbol{x}_i^T\boldsymbol{\beta})^2
\]</span></p>
</section>
<section id="why-square-the-errors" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="why-square-the-errors"><span class="header-section-number">8.2</span> Why Square the Errors?</h2>
<p>We could use different error metrics. Why do we square the errors?</p>
<ol type="1">
<li><p><strong>Penalizes large errors more</strong>: An error of 2 contributes 4 to RSS, while two errors of 1 each contribute only 2. This makes the model sensitive to outliers.</p></li>
<li><p><strong>Mathematical tractability</strong>: Squared errors lead to a smooth, differentiable objective function with a unique minimum.</p></li>
<li><p><strong>Statistical optimality</strong>: Under Gaussian noise, least squares is the maximum likelihood estimator.</p></li>
<li><p><strong>Geometric interpretation</strong>: Minimizing RSS is equivalent to finding the projection onto the column space of <span class="math inline">\(\boldsymbol{X}\)</span>.</p></li>
<li><p><strong>No sign cancellation</strong>: Unlike simple sum of errors (where positive and negative errors cancel), squaring ensures all errors contribute positively.</p></li>
</ol>
</section>
<section id="matrix-form" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="matrix-form"><span class="header-section-number">8.3</span> Matrix Form</h2>
<p>In matrix-vector notation:</p>
<p><span class="math display">\[
\text{RSS}(\boldsymbol{\beta}) = \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2 = (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})
\]</span></p>
<p>This is the squared Euclidean norm of the residual vector <span class="math inline">\(\boldsymbol{r} = \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\)</span>.</p>
</section>
<section id="the-optimization-goal" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="the-optimization-goal"><span class="header-section-number">8.4</span> The Optimization Goal</h2>
<p>Our objective is to find the parameters that minimize RSS:</p>
<p><span class="math display">\[
\boldsymbol{\beta}^\ast = \arg\min_{\boldsymbol{\beta}} \text{RSS}(\boldsymbol{\beta}) = \arg\min_{\boldsymbol{\beta}} \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2
\]</span></p>
<p><strong>Physical Interpretation</strong>: In the context of aircraft performance modeling, we’re finding the model parameters that minimize the total squared difference between predicted and actual drag coefficients across all wind tunnel measurements. This gives us the “best fit” in a least-squares sense.</p>
</section>
</section>
<section id="derivation-normal-equations" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Derivation: Normal Equations</h1>
<p>We can derive the optimal solution using calculus. The approach is to expand the objective function, take the derivative with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, set it to zero, and solve.</p>
<section id="step-1-expand-the-objective-function" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="step-1-expand-the-objective-function"><span class="header-section-number">9.1</span> Step 1: Expand the Objective Function</h2>
<p>Starting with: <span class="math display">\[
\text{RSS}(\boldsymbol{\beta}) = (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})
\]</span></p>
<p>Expand: <span class="math display">\[
\text{RSS}(\boldsymbol{\beta}) = \boldsymbol{y}^T\boldsymbol{y} - \boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{y} + \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}
\]</span></p>
<p>Since <span class="math inline">\(\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta}\)</span> is a scalar, it equals its transpose: <span class="math inline">\(\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta} = \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{y}\)</span></p>
<p>Therefore: <span class="math display">\[
\text{RSS}(\boldsymbol{\beta}) = \boldsymbol{y}^T\boldsymbol{y} - 2\boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{y} + \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}
\]</span></p>
</section>
<section id="step-2-take-derivative-with-respect-to-boldsymbolbeta" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="step-2-take-derivative-with-respect-to-boldsymbolbeta"><span class="header-section-number">9.2</span> Step 2: Take Derivative with Respect to <span class="math inline">\(\boldsymbol{\beta}\)</span></h2>
<p>Using matrix calculus identities: - <span class="math inline">\(\frac{\partial}{\partial \boldsymbol{\beta}}(\boldsymbol{a}^T\boldsymbol{\beta}) = \boldsymbol{a}\)</span> - <span class="math inline">\(\frac{\partial}{\partial \boldsymbol{\beta}}(\boldsymbol{\beta}^T\boldsymbol{A}\boldsymbol{\beta}) = 2\boldsymbol{A}\boldsymbol{\beta}\)</span> (when <span class="math inline">\(\boldsymbol{A}\)</span> is symmetric)</p>
<p>We get: <span class="math display">\[
\frac{\partial \text{RSS}}{\partial \boldsymbol{\beta}} = -2\boldsymbol{X}^T\boldsymbol{y} + 2\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}
\]</span></p>
</section>
<section id="step-3-set-to-zero-and-solve" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="step-3-set-to-zero-and-solve"><span class="header-section-number">9.3</span> Step 3: Set to Zero and Solve</h2>
<p>At the minimum, the gradient must be zero: <span class="math display">\[
-2\boldsymbol{X}^T\boldsymbol{y} + 2\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}^\ast = \boldsymbol{0}
\]</span></p>
<p>Dividing by 2 and rearranging: <span class="math display">\[
\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}^\ast = \boldsymbol{X}^T\boldsymbol{y}
\]</span></p>
<p>These are called the <strong>Normal Equations</strong>.</p>
</section>
<section id="step-4-closed-form-solution" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="step-4-closed-form-solution"><span class="header-section-number">9.4</span> Step 4: Closed-Form Solution</h2>
<p>If <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> is invertible (which requires that the columns of <span class="math inline">\(\boldsymbol{X}\)</span> are linearly independent), we can solve for <span class="math inline">\(\boldsymbol{\beta}^\ast\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\beta}^\ast = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\]</span></p>
<p>This is the <strong>ordinary least squares (OLS) solution</strong>.</p>
<p><strong>Verification that this is a minimum</strong>: The second derivative (Hessian matrix) is: <span class="math display">\[
\frac{\partial^2 \text{RSS}}{\partial \boldsymbol{\beta}^2} = 2\boldsymbol{X}^T\boldsymbol{X}
\]</span></p>
<p>This is positive definite (assuming <span class="math inline">\(\boldsymbol{X}\)</span> has full column rank), confirming that we have found a minimum, not a maximum or saddle point.</p>
</section>
</section>
<section id="geometric-interpretation-understanding-the-error" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Geometric Interpretation: Understanding the Error</h1>
<section id="what-is-the-residual-vector" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="what-is-the-residual-vector"><span class="header-section-number">10.1</span> What is the Residual Vector?</h2>
<p>The residual (error) vector is defined as: <span class="math display">\[
\boldsymbol{r} = \boldsymbol{y} - \hat{\boldsymbol{y}} = \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}
\]</span></p>
<p>This is the vector of differences between: - <strong>Observed data</strong> <span class="math inline">\(\boldsymbol{y}\)</span>: What we actually measured - <strong>Predictions</strong> <span class="math inline">\(\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}\)</span>: What our model predicts</p>
</section>
<section id="goal-minimize-the-length-of-the-error-vector" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="goal-minimize-the-length-of-the-error-vector"><span class="header-section-number">10.2</span> Goal: Minimize the Length of the Error Vector</h2>
<p>Our optimization problem can be stated as: <span class="math display">\[
\min_{\boldsymbol{\beta}} \|\boldsymbol{r}\|^2 = \min_{\boldsymbol{\beta}} \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2
\]</span></p>
<p>We want to make the residual vector as short as possible.</p>
</section>
<section id="column-space-of-boldsymbolx" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="column-space-of-boldsymbolx"><span class="header-section-number">10.3</span> Column Space of <span class="math inline">\(\boldsymbol{X}\)</span></h2>
<p><strong>Definition</strong>: The column space of <span class="math inline">\(\boldsymbol{X}\)</span>, denoted col(<span class="math inline">\(\boldsymbol{X}\)</span>), is the set of all possible linear combinations of the columns of <span class="math inline">\(\boldsymbol{X}\)</span>:</p>
<p><span class="math display">\[
\text{col}(\boldsymbol{X}) = \{\boldsymbol{X}\boldsymbol{\beta} : \boldsymbol{\beta} \in \mathbb{R}^{d+1}\}
\]</span></p>
<p><strong>Key Insight</strong>: - Every prediction <span class="math inline">\(\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}\)</span> <strong>must lie in</strong> col(<span class="math inline">\(\boldsymbol{X}\)</span>) - The observed data <span class="math inline">\(\boldsymbol{y}\)</span> typically does <strong>not</strong> lie in col(<span class="math inline">\(\boldsymbol{X}\)</span>) (due to measurement noise) - We seek the <strong>best approximation</strong> to <span class="math inline">\(\boldsymbol{y}\)</span> within col(<span class="math inline">\(\boldsymbol{X}\)</span>)</p>
</section>
<section id="the-fundamental-geometric-principle" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="the-fundamental-geometric-principle"><span class="header-section-number">10.4</span> The Fundamental Geometric Principle</h2>
<p><strong>Question</strong>: What is the closest point in col(<span class="math inline">\(\boldsymbol{X}\)</span>) to <span class="math inline">\(\boldsymbol{y}\)</span>?</p>
<p><strong>Answer</strong>: The <strong>orthogonal projection</strong> of <span class="math inline">\(\boldsymbol{y}\)</span> onto col(<span class="math inline">\(\boldsymbol{X}\)</span>).</p>
<p><strong>Why?</strong> The <strong>Projection Theorem</strong> from linear algebra states: The shortest distance from a point to a subspace is achieved by the perpendicular projection onto that subspace.</p>
<p><strong>Mathematical Statement</strong>: The error is minimized when it is orthogonal to the column space: <span class="math display">\[
\boldsymbol{r} \perp \text{col}(\boldsymbol{X}) \quad \Longleftrightarrow \quad \boldsymbol{X}^T\boldsymbol{r} = \boldsymbol{0}
\]</span></p>
<p>This orthogonality condition is the <strong>geometric essence</strong> of least squares.</p>
</section>
</section>
<section id="why-projection-minimizes-error" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Why Projection Minimizes Error</h1>
<section id="visualizing-the-projection" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="visualizing-the-projection"><span class="header-section-number">11.1</span> Visualizing the Projection</h2>
<p>Imagine a 2D plane (representing col(<span class="math inline">\(\boldsymbol{X}\)</span>)) embedded in 3D space (representing <span class="math inline">\(\mathbb{R}^n\)</span>). The data vector <span class="math inline">\(\boldsymbol{y}\)</span> is a point not on this plane.</p>
<p><strong>Key Observations</strong>:</p>
<ol type="1">
<li>Any prediction <span class="math inline">\(\hat{\boldsymbol{y}}\)</span> must lie on the plane (in col(<span class="math inline">\(\boldsymbol{X}\)</span>))</li>
<li>The error <span class="math inline">\(\boldsymbol{r} = \boldsymbol{y} - \hat{\boldsymbol{y}}\)</span> is the vector from <span class="math inline">\(\hat{\boldsymbol{y}}\)</span> to <span class="math inline">\(\boldsymbol{y}\)</span></li>
<li>We want to minimize <span class="math inline">\(\|\boldsymbol{r}\|\)</span> (the length of this vector)</li>
</ol>
</section>
<section id="why-perpendicular-is-optimal" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="why-perpendicular-is-optimal"><span class="header-section-number">11.2</span> Why Perpendicular is Optimal</h2>
<p>Consider any two points in col(<span class="math inline">\(\boldsymbol{X}\)</span>): - <span class="math inline">\(\hat{\boldsymbol{y}}_\perp\)</span>: The perpendicular projection - <span class="math inline">\(\hat{\boldsymbol{y}}_{\text{other}}\)</span>: Any other point</p>
<p>By the <strong>Pythagorean theorem</strong>: <span class="math display">\[
\|\boldsymbol{y} - \hat{\boldsymbol{y}}_{\text{other}}\|^2 = \|\boldsymbol{y} - \hat{\boldsymbol{y}}_\perp\|^2 + \|\hat{\boldsymbol{y}}_\perp - \hat{\boldsymbol{y}}_{\text{other}}\|^2
\]</span></p>
<p>Since <span class="math inline">\(\|\hat{\boldsymbol{y}}_\perp - \hat{\boldsymbol{y}}_{\text{other}}\|^2 \geq 0\)</span>, we have: <span class="math display">\[
\|\boldsymbol{y} - \hat{\boldsymbol{y}}_{\text{other}}\|^2 \geq \|\boldsymbol{y} - \hat{\boldsymbol{y}}_\perp\|^2
\]</span></p>
<p>Therefore, the perpendicular projection gives the smallest error.</p>
</section>
<section id="mathematical-statement-of-orthogonality" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="mathematical-statement-of-orthogonality"><span class="header-section-number">11.3</span> Mathematical Statement of Orthogonality</h2>
<p>The optimal prediction <span class="math inline">\(\hat{\boldsymbol{y}}^\ast\)</span> satisfies: <span class="math display">\[
(\boldsymbol{y} - \hat{\boldsymbol{y}}^\ast) \perp \text{col}(\boldsymbol{X})
\]</span></p>
<p>In matrix form: <span class="math display">\[
\boldsymbol{X}^T(\boldsymbol{y} - \hat{\boldsymbol{y}}^\ast) = \boldsymbol{0}
\]</span></p>
<p><strong>Understanding <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{r} = \boldsymbol{0}\)</span></strong>:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{X}^T\boldsymbol{r}\)</span> is a vector of dot products: <span class="math inline">\([\boldsymbol{x}_1^T\boldsymbol{r}, \boldsymbol{x}_2^T\boldsymbol{r}, \ldots, \boldsymbol{x}_{d+1}^T\boldsymbol{r}]^T\)</span></li>
<li>Each dot product equals zero: <span class="math inline">\(\boldsymbol{x}_j^T\boldsymbol{r} = 0\)</span></li>
<li>This means <span class="math inline">\(\boldsymbol{r}\)</span> is perpendicular to <strong>every column</strong> of <span class="math inline">\(\boldsymbol{X}\)</span></li>
<li>Therefore, <span class="math inline">\(\boldsymbol{r}\)</span> is perpendicular to the <strong>entire column space</strong></li>
</ul>
</section>
</section>
<section id="deriving-the-optimal-solution-via-projection" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Deriving the Optimal Solution via Projection</h1>
<p>Rather than using calculus, we can derive the OLS solution directly from the geometric principle of orthogonal projection.</p>
<section id="step-1-state-the-orthogonality-condition" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="step-1-state-the-orthogonality-condition"><span class="header-section-number">12.1</span> Step 1: State the Orthogonality Condition</h2>
<p>From geometry, the error must be perpendicular to col(<span class="math inline">\(\boldsymbol{X}\)</span>): <span class="math display">\[
\boldsymbol{r} \perp \text{col}(\boldsymbol{X}) \quad \Longrightarrow \quad \boldsymbol{X}^T\boldsymbol{r} = \boldsymbol{0}
\]</span></p>
<p>This is <strong>the fundamental condition</strong> for least squares optimality.</p>
</section>
<section id="step-2-express-in-terms-of-boldsymbolbeta" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="step-2-express-in-terms-of-boldsymbolbeta"><span class="header-section-number">12.2</span> Step 2: Express in Terms of <span class="math inline">\(\boldsymbol{\beta}\)</span></h2>
<p>The optimal prediction is <span class="math inline">\(\hat{\boldsymbol{y}}^\ast = \boldsymbol{X}\boldsymbol{\beta}^\ast\)</span> and the optimal residual is <span class="math inline">\(\boldsymbol{r}^\ast = \boldsymbol{y} - \hat{\boldsymbol{y}}^\ast\)</span>.</p>
<p>Substituting into the orthogonality condition: <span class="math display">\[
\boldsymbol{X}^T\boldsymbol{r}^\ast = \boldsymbol{0}
\]</span></p>
<p><span class="math display">\[
\boldsymbol{X}^T(\boldsymbol{y} - \hat{\boldsymbol{y}}^\ast) = \boldsymbol{0}
\]</span></p>
<p><span class="math display">\[
\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}^\ast) = \boldsymbol{0}
\]</span></p>
</section>
<section id="step-3-derive-the-normal-equations" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="step-3-derive-the-normal-equations"><span class="header-section-number">12.3</span> Step 3: Derive the Normal Equations</h2>
<p>Expanding: <span class="math display">\[
\boldsymbol{X}^T\boldsymbol{y} - \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}^\ast = \boldsymbol{0}
\]</span></p>
<p>Rearranging: <span class="math display">\[
\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}^\ast = \boldsymbol{X}^T\boldsymbol{y}
\]</span></p>
<p>These are the <strong>Normal Equations</strong>—exactly the same result we obtained using calculus!</p>
</section>
<section id="step-4-solve-for-boldsymbolbetaast" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="step-4-solve-for-boldsymbolbetaast"><span class="header-section-number">12.4</span> Step 4: Solve for <span class="math inline">\(\boldsymbol{\beta}^\ast\)</span></h2>
<p>Assuming <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> is invertible: <span class="math display">\[
\boldsymbol{\beta}^\ast = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\]</span></p>
<p><strong>Key Insight</strong>: We derived the OLS solution using <strong>projection geometry</strong> (orthogonality condition) instead of <strong>calculus</strong> (setting derivative to zero). Both paths lead to the same answer, providing two complementary perspectives:</p>
<ul>
<li><strong>Calculus view</strong>: <span class="math inline">\(\boldsymbol{\beta}^\ast\)</span> minimizes the cost function</li>
<li><strong>Geometric view</strong>: <span class="math inline">\(\boldsymbol{\beta}^\ast\)</span> gives the perpendicular projection</li>
</ul>
</section>
</section>
<section id="the-projection-matrix" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> The Projection Matrix</h1>
<section id="definition" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">13.1</span> Definition</h2>
<p>The <strong>projection matrix</strong> is defined as: <span class="math display">\[
\boldsymbol{P} = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T
\]</span></p>
<p>This matrix projects any vector in <span class="math inline">\(\mathbb{R}^n\)</span> onto col(<span class="math inline">\(\boldsymbol{X}\)</span>).</p>
</section>
<section id="computing-predictions" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="computing-predictions"><span class="header-section-number">13.2</span> Computing Predictions</h2>
<p>Using the projection matrix, we can write the predicted values as: <span class="math display">\[
\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}^\ast = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y} = \boldsymbol{P}\boldsymbol{y}
\]</span></p>
<p><strong>Interpretation</strong>: <span class="math inline">\(\boldsymbol{P}\)</span> directly maps observed data <span class="math inline">\(\boldsymbol{y}\)</span> to predictions <span class="math inline">\(\hat{\boldsymbol{y}}\)</span>, bypassing the need to explicitly compute <span class="math inline">\(\boldsymbol{\beta}^\ast\)</span>.</p>
</section>
<section id="key-properties" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="key-properties"><span class="header-section-number">13.3</span> Key Properties</h2>
<section id="idempotent-projecting-twice-projecting-once" class="level3" data-number="13.3.1">
<h3 data-number="13.3.1" class="anchored" data-anchor-id="idempotent-projecting-twice-projecting-once"><span class="header-section-number">13.3.1</span> 1. Idempotent (Projecting Twice = Projecting Once)</h3>
<p><span class="math display">\[
\boldsymbol{P}^2 = \boldsymbol{P}
\]</span></p>
<p><strong>Proof</strong>: <span class="math display">\[
\boldsymbol{P}^2 = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T = \boldsymbol{P}
\]</span></p>
<p><strong>Interpretation</strong>: If a vector is already in col(<span class="math inline">\(\boldsymbol{X}\)</span>), projecting it again doesn’t change it.</p>
</section>
<section id="symmetric" class="level3" data-number="13.3.2">
<h3 data-number="13.3.2" class="anchored" data-anchor-id="symmetric"><span class="header-section-number">13.3.2</span> 2. Symmetric</h3>
<p><span class="math display">\[
\boldsymbol{P}^T = \boldsymbol{P}
\]</span></p>
<p><strong>Proof</strong>: Each factor is symmetric: <span class="math display">\[
\boldsymbol{P}^T = (\boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T)^T = \boldsymbol{X}((\boldsymbol{X}^T\boldsymbol{X})^{-1})^T\boldsymbol{X}^T = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T = \boldsymbol{P}
\]</span></p>
</section>
<section id="projects-onto-colboldsymbolx" class="level3" data-number="13.3.3">
<h3 data-number="13.3.3" class="anchored" data-anchor-id="projects-onto-colboldsymbolx"><span class="header-section-number">13.3.3</span> 3. Projects onto col(<span class="math inline">\(\boldsymbol{X}\)</span>)</h3>
<p><span class="math display">\[
\boldsymbol{P}\boldsymbol{X} = \boldsymbol{X}
\]</span></p>
<p><strong>Proof</strong>: <span class="math display">\[
\boldsymbol{P}\boldsymbol{X} = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{X} = \boldsymbol{X}
\]</span></p>
<p><strong>Interpretation</strong>: Any vector in col(<span class="math inline">\(\boldsymbol{X}\)</span>) is unchanged by the projection.</p>
</section>
<section id="residual-matrix" class="level3" data-number="13.3.4">
<h3 data-number="13.3.4" class="anchored" data-anchor-id="residual-matrix"><span class="header-section-number">13.3.4</span> 4. Residual Matrix</h3>
<p><span class="math display">\[
\boldsymbol{I} - \boldsymbol{P}
\]</span></p>
<p>This matrix projects onto the orthogonal complement of col(<span class="math inline">\(\boldsymbol{X}\)</span>), giving the residuals: <span class="math display">\[
\boldsymbol{r} = (\boldsymbol{I} - \boldsymbol{P})\boldsymbol{y}
\]</span></p>
</section>
</section>
<section id="aerospace-interpretation" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="aerospace-interpretation"><span class="header-section-number">13.4</span> Aerospace Interpretation</h2>
<p>In the context of drag coefficient prediction:</p>
<ul>
<li><strong><span class="math inline">\(\boldsymbol{P}\boldsymbol{y}\)</span></strong>: The component of observed drag that <strong>can be explained</strong> by our aerodynamic features (angle of attack, Mach number, etc.)</li>
<li><strong><span class="math inline">\((\boldsymbol{I} - \boldsymbol{P})\boldsymbol{y}\)</span></strong>: The component that <strong>cannot be explained</strong> (residual variance due to unmodeled effects, measurement noise, etc.)</li>
</ul>
<p>The projection matrix decomposes the total variance into: - <strong>Explained variance</strong> (signal captured by the model) - <strong>Unexplained variance</strong> (noise or missing features)</p>
</section>
</section>
<section id="when-direct-solution-fails" class="level1" data-number="14">
<h1 data-number="14"><span class="header-section-number">14</span> When Direct Solution Fails</h1>
<p>While the closed-form solution <span class="math inline">\(\boldsymbol{\beta}^\ast = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}\)</span> is elegant, it faces several practical challenges.</p>
<section id="problem-1-singular-matrix-non-invertibility" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="problem-1-singular-matrix-non-invertibility"><span class="header-section-number">14.1</span> Problem 1: Singular Matrix (Non-Invertibility)</h2>
<p><strong>When it occurs</strong>:</p>
<ol type="1">
<li><strong>More features than samples</strong>: <span class="math inline">\(n &lt; d+1\)</span>
<ul>
<li>Not enough data to uniquely determine all parameters</li>
<li>Infinitely many solutions exist</li>
<li>Example: 50 wind tunnel measurements but 100 polynomial features</li>
</ul></li>
<li><strong>Multicollinearity</strong>: Highly correlated features
<ul>
<li>Example: Including both velocity and Mach number when temperature is constant (they’re perfectly correlated)</li>
<li><span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> becomes nearly singular (very small eigenvalues)</li>
<li>Small numerical errors can cause huge changes in solution</li>
</ul></li>
</ol>
<p><strong>Consequences</strong>: - Cannot compute <span class="math inline">\((\boldsymbol{X}^T\boldsymbol{X})^{-1}\)</span> - Solution is non-unique or unstable</p>
</section>
<section id="problem-2-computational-cost" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="problem-2-computational-cost"><span class="header-section-number">14.2</span> Problem 2: Computational Cost</h2>
<p><strong>Matrix inversion complexity</strong>: <span class="math inline">\(O(d^3)\)</span> operations</p>
<p>For high-dimensional problems: - <span class="math inline">\(d = 1000\)</span> features: ~1 billion operations - <span class="math inline">\(d = 10000\)</span> features: ~1 trillion operations</p>
<p><strong>Memory requirements</strong>: Storing <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> requires <span class="math inline">\(O(d^2)\)</span> memory</p>
<p><strong>When it matters</strong>: - Large-scale machine learning (millions of features) - Online learning (real-time updates) - Embedded systems (limited computational resources)</p>
</section>
<section id="problem-3-numerical-stability" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="problem-3-numerical-stability"><span class="header-section-number">14.3</span> Problem 3: Numerical Stability</h2>
<p><strong>Condition number</strong>: <span class="math inline">\(\kappa(\boldsymbol{X}^T\boldsymbol{X}) = \frac{\sigma_{\max}}{\sigma_{\min}}\)</span></p>
<ul>
<li>Large condition number → small perturbations in data cause large changes in solution</li>
<li>Floating-point arithmetic errors accumulate</li>
<li>Results become unreliable</li>
</ul>
<p><strong>Example</strong>: If <span class="math inline">\(\kappa = 10^{10}\)</span> and we use 64-bit floating point (16 digits precision), we may lose all significant digits in the solution.</p>
</section>
<section id="solutions" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="solutions"><span class="header-section-number">14.4</span> Solutions</h2>
<section id="regularization" class="level3" data-number="14.4.1">
<h3 data-number="14.4.1" class="anchored" data-anchor-id="regularization"><span class="header-section-number">14.4.1</span> 1. Regularization</h3>
<p>Add a penalty term to prevent overfitting and improve conditioning:</p>
<p><strong>Ridge Regression (L2)</strong>: <span class="math display">\[
\boldsymbol{\beta}^\ast = (\boldsymbol{X}^T\boldsymbol{X} + \lambda \boldsymbol{I})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\]</span></p>
<p><strong>Lasso (L1)</strong>: <span class="math display">\[
\min_{\boldsymbol{\beta}} \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1
\]</span></p>
</section>
<section id="gradient-descent" class="level3" data-number="14.4.2">
<h3 data-number="14.4.2" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">14.4.2</span> 2. Gradient Descent</h3>
<p>Iteratively update parameters without matrix inversion: - Complexity per iteration: <span class="math inline">\(O(nd)\)</span> instead of <span class="math inline">\(O(d^3)\)</span> - Can handle very large <span class="math inline">\(d\)</span> - Works for any differentiable loss function</p>
</section>
<section id="qr-decomposition" class="level3" data-number="14.4.3">
<h3 data-number="14.4.3" class="anchored" data-anchor-id="qr-decomposition"><span class="header-section-number">14.4.3</span> 3. QR Decomposition</h3>
<p>Numerically stable direct method: <span class="math display">\[
\boldsymbol{X} = \boldsymbol{Q}\boldsymbol{R}
\]</span> - More stable than normal equations - Still <span class="math inline">\(O(nd^2)\)</span> complexity</p>
</section>
<section id="singular-value-decomposition-svd" class="level3" data-number="14.4.4">
<h3 data-number="14.4.4" class="anchored" data-anchor-id="singular-value-decomposition-svd"><span class="header-section-number">14.4.4</span> 4. Singular Value Decomposition (SVD)</h3>
<p>Most numerically stable method: <span class="math display">\[
\boldsymbol{X} = \boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T
\]</span> - Handles rank deficiency gracefully - Reveals multicollinearity - Gold standard for numerical stability</p>
</section>
</section>
</section>
<section id="gradient-descent-iterative-approach" class="level1" data-number="15">
<h1 data-number="15"><span class="header-section-number">15</span> Gradient Descent: Iterative Approach</h1>
<p>When direct solution is impractical, we can use iterative optimization methods. Gradient descent is the foundation of modern machine learning.</p>
<section id="the-algorithm" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="the-algorithm"><span class="header-section-number">15.1</span> The Algorithm</h2>
<p><strong>Goal</strong>: Minimize <span class="math inline">\(\text{RSS}(\boldsymbol{\beta}) = \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2\)</span></p>
<p><strong>Strategy</strong>: Start with an initial guess and repeatedly move in the direction of steepest descent.</p>
<section id="steps" class="level3" data-number="15.1.1">
<h3 data-number="15.1.1" class="anchored" data-anchor-id="steps"><span class="header-section-number">15.1.1</span> Steps</h3>
<ol type="1">
<li><p><strong>Initialize</strong>: Choose starting parameters <span class="math inline">\(\boldsymbol{\beta}^{(0)}\)</span> (typically random or zeros)</p></li>
<li><p><strong>Iterate</strong>: For <span class="math inline">\(t = 0, 1, 2, \ldots\)</span> until convergence: <span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \nabla_{\boldsymbol{\beta}} \text{RSS}(\boldsymbol{\beta}^{(t)})
\]</span></p>
<p>where <span class="math inline">\(\eta &gt; 0\)</span> is the <strong>learning rate</strong> (step size)</p></li>
<li><p><strong>Stop</strong>: When change is small enough or maximum iterations reached</p></li>
</ol>
</section>
</section>
<section id="computing-the-gradient" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="computing-the-gradient"><span class="header-section-number">15.2</span> Computing the Gradient</h2>
<p>The gradient of RSS is: <span class="math display">\[
\nabla_{\boldsymbol{\beta}} \text{RSS} = \frac{\partial}{\partial \boldsymbol{\beta}} \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2 = -2\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})
\]</span></p>
<p><strong>Update Rule</strong>: <span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + 2\eta \boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}^{(t)})
\]</span></p>
<p><strong>Intuition</strong>: - The gradient <span class="math inline">\(\nabla \text{RSS}\)</span> points in the direction of steepest <strong>increase</strong> - We move in the <strong>opposite direction</strong> (negative gradient) to decrease the error - The learning rate <span class="math inline">\(\eta\)</span> controls how big a step we take</p>
</section>
<section id="convergence" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="convergence"><span class="header-section-number">15.3</span> Convergence</h2>
<p><strong>Under appropriate conditions</strong> (convexity, suitable learning rate): <span class="math display">\[
\boldsymbol{\beta}^{(t)} \to \boldsymbol{\beta}^\ast \quad \text{as} \quad t \to \infty
\]</span></p>
<p>The method converges to the same solution as the direct method, but arrives there iteratively.</p>
</section>
</section>
<section id="gradient-descent-variants" class="level1" data-number="16">
<h1 data-number="16"><span class="header-section-number">16</span> Gradient Descent Variants</h1>
<p>Different variants of gradient descent trade off between convergence speed and computational cost per iteration.</p>
<section id="batch-gradient-descent" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="batch-gradient-descent"><span class="header-section-number">16.1</span> Batch Gradient Descent</h2>
<p><strong>Use all <span class="math inline">\(n\)</span> samples</strong> in each iteration: <span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \nabla_{\boldsymbol{\beta}} \text{RSS}(\boldsymbol{\beta}^{(t)})
\]</span></p>
<p>where the gradient uses all data: <span class="math display">\[
\nabla_{\boldsymbol{\beta}} \text{RSS} = -2\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}^{(t)}) = -2\sum_{i=1}^n \boldsymbol{x}_i(y_i - \boldsymbol{x}_i^T\boldsymbol{\beta}^{(t)})
\]</span></p>
<p><strong>Advantages</strong>: - Stable, smooth convergence - Gradient is exact (no sampling noise) - Can use optimized linear algebra libraries</p>
<p><strong>Disadvantages</strong>: - Each iteration costs <span class="math inline">\(O(nd)\)</span> - Slow for very large datasets (<span class="math inline">\(n\)</span> in millions) - Entire dataset must fit in memory</p>
<p><strong>When to use</strong>: Small to medium datasets, when you need stable convergence</p>
</section>
<section id="stochastic-gradient-descent-sgd" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="stochastic-gradient-descent-sgd"><span class="header-section-number">16.2</span> Stochastic Gradient Descent (SGD)</h2>
<p><strong>Use one random sample</strong> <span class="math inline">\(i\)</span> per iteration: <span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + 2\eta \boldsymbol{x}_i(y_i - \boldsymbol{x}_i^T\boldsymbol{\beta}^{(t)})
\]</span></p>
<p><strong>Advantages</strong>: - Very fast updates: <span class="math inline">\(O(d)\)</span> per iteration - Can process data online (streaming) - May escape shallow local minima (for non-convex problems) - Naturally handles huge datasets</p>
<p><strong>Disadvantages</strong>: - Noisy updates (high variance) - Oscillates around minimum (never truly converges) - Requires careful learning rate tuning - May need learning rate decay</p>
<p><strong>When to use</strong>: Very large datasets, online learning, when fast iteration is more important than stable convergence</p>
</section>
<section id="mini-batch-gradient-descent" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="mini-batch-gradient-descent"><span class="header-section-number">16.3</span> Mini-Batch Gradient Descent</h2>
<p><strong>Use a random subset</strong> of <span class="math inline">\(b\)</span> samples per iteration (typical: <span class="math inline">\(b = 32, 64, 128, 256\)</span>): <span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \frac{1}{b}\sum_{i \in \mathcal{B}_t} -2\boldsymbol{x}_i(y_i - \boldsymbol{x}_i^T\boldsymbol{\beta}^{(t)})
\]</span></p>
<p>where <span class="math inline">\(\mathcal{B}_t\)</span> is a random mini-batch at iteration <span class="math inline">\(t\)</span>.</p>
<p><strong>Advantages</strong>: - <strong>Best balance</strong> between speed and stability - Vectorized operations (GPU-friendly) - Reduced variance compared to SGD - Faster than batch for large <span class="math inline">\(n\)</span></p>
<p><strong>Disadvantages</strong>: - One more hyperparameter (batch size) - Still some oscillation</p>
<p><strong>When to use</strong>: <strong>Default choice for modern machine learning</strong>—combines benefits of both batch and stochastic methods</p>
</section>
<section id="comparison-summary" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="comparison-summary"><span class="header-section-number">16.4</span> Comparison Summary</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 30%">
<col style="width: 26%">
<col style="width: 18%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Samples per Iteration</th>
<th>Cost per Iteration</th>
<th>Convergence</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Batch</td>
<td>All (<span class="math inline">\(n\)</span>)</td>
<td><span class="math inline">\(O(nd)\)</span></td>
<td>Smooth, stable</td>
<td>Small datasets</td>
</tr>
<tr class="even">
<td>Stochastic</td>
<td>1</td>
<td><span class="math inline">\(O(d)\)</span></td>
<td>Noisy, fast</td>
<td>Huge datasets, online</td>
</tr>
<tr class="odd">
<td>Mini-batch</td>
<td><span class="math inline">\(b\)</span> (32-256)</td>
<td><span class="math inline">\(O(bd)\)</span></td>
<td>Balanced</td>
<td><strong>Most applications</strong></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="learning-rate-selection" class="level1" data-number="17">
<h1 data-number="17"><span class="header-section-number">17</span> Learning Rate Selection</h1>
<p>The learning rate <span class="math inline">\(\eta\)</span> is one of the most critical hyperparameters in gradient descent.</p>
<section id="the-role-of-learning-rate" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="the-role-of-learning-rate"><span class="header-section-number">17.1</span> The Role of Learning Rate</h2>
<p>In the update rule: <span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \nabla_{\boldsymbol{\beta}} \text{RSS}(\boldsymbol{\beta}^{(t)})
\]</span></p>
<p><span class="math inline">\(\eta\)</span> controls <strong>how far</strong> we move in the direction of the negative gradient.</p>
</section>
<section id="too-small-learning-rate-eta-ll-1" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="too-small-learning-rate-eta-ll-1"><span class="header-section-number">17.2</span> Too Small Learning Rate (<span class="math inline">\(\eta \ll 1\)</span>)</h2>
<p><strong>Symptoms</strong>: - Very slow progress toward minimum - Requires many iterations to converge - May time out before reaching optimum</p>
<p><strong>Cost</strong>: Wasted computation time</p>
<p><strong>Example</strong>: If optimal <span class="math inline">\(\eta = 0.01\)</span> but we use <span class="math inline">\(\eta = 0.0001\)</span>, we need 100× more iterations.</p>
</section>
<section id="too-large-learning-rate-eta-gg-1" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="too-large-learning-rate-eta-gg-1"><span class="header-section-number">17.3</span> Too Large Learning Rate (<span class="math inline">\(\eta \gg 1\)</span>)</h2>
<p><strong>Symptoms</strong>: - Overshooting the minimum - Oscillation around optimum - Divergence (error increases instead of decreasing) - Instability</p>
<p><strong>Cost</strong>: Never converges, wasted effort</p>
<p><strong>Example</strong>: Parameters “bounce” past the minimum on each side, never settling down.</p>
</section>
<section id="finding-the-right-learning-rate" class="level2" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="finding-the-right-learning-rate"><span class="header-section-number">17.4</span> Finding the Right Learning Rate</h2>
<p><strong>Rule of thumb</strong>: Start with <span class="math inline">\(\eta \in \{0.001, 0.01, 0.1, 1.0\}\)</span> and tune based on training curves.</p>
<p><strong>Grid search</strong>: Try multiple values, plot RSS vs.&nbsp;iteration, choose the largest <span class="math inline">\(\eta\)</span> that converges smoothly.</p>
<p><strong>Diagnostic</strong>: Plot <span class="math inline">\(\text{RSS}^{(t)}\)</span> vs.&nbsp;<span class="math inline">\(t\)</span>: - Decreasing smoothly → good - Decreasing then oscillating → too large - Flat or barely decreasing → too small - Increasing → way too large</p>
</section>
<section id="adaptive-learning-rates" class="level2" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="adaptive-learning-rates"><span class="header-section-number">17.5</span> Adaptive Learning Rates</h2>
<p>Modern optimization uses sophisticated learning rate strategies:</p>
<section id="learning-rate-decay" class="level3" data-number="17.5.1">
<h3 data-number="17.5.1" class="anchored" data-anchor-id="learning-rate-decay"><span class="header-section-number">17.5.1</span> 1. Learning Rate Decay</h3>
<p>Decrease <span class="math inline">\(\eta\)</span> over time: <span class="math display">\[
\eta_t = \frac{\eta_0}{1 + kt}
\]</span></p>
<p>or exponential decay: <span class="math display">\[
\eta_t = \eta_0 e^{-kt}
\]</span></p>
<p><strong>Rationale</strong>: Start with large steps (fast progress), then small steps (fine-tuning near minimum)</p>
</section>
<section id="momentum" class="level3" data-number="17.5.2">
<h3 data-number="17.5.2" class="anchored" data-anchor-id="momentum"><span class="header-section-number">17.5.2</span> 2. Momentum</h3>
<p>Use exponentially weighted moving average of gradients: <span class="math display">\[
\boldsymbol{v}^{(t)} = \gamma \boldsymbol{v}^{(t-1)} + \eta \nabla_{\boldsymbol{\beta}} \text{RSS}(\boldsymbol{\beta}^{(t)})
\]</span> <span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \boldsymbol{v}^{(t)}
\]</span></p>
<p><strong>Benefits</strong>: - Accelerates convergence in relevant directions - Dampens oscillations - Helps escape plateaus</p>
</section>
<section id="adam-adaptive-moment-estimation" class="level3" data-number="17.5.3">
<h3 data-number="17.5.3" class="anchored" data-anchor-id="adam-adaptive-moment-estimation"><span class="header-section-number">17.5.3</span> 3. Adam (Adaptive Moment Estimation)</h3>
<p><strong>Most popular modern optimizer</strong>. Combines: - Momentum (first moment) - Adaptive learning rates per parameter (second moment)</p>
<p><strong>Update rule</strong> (simplified): <span class="math display">\[
\boldsymbol{m}^{(t)} = \beta_1 \boldsymbol{m}^{(t-1)} + (1-\beta_1)\boldsymbol{g}^{(t)}
\]</span> <span class="math display">\[
\boldsymbol{v}^{(t)} = \beta_2 \boldsymbol{v}^{(t-1)} + (1-\beta_2)(\boldsymbol{g}^{(t)})^2
\]</span> <span class="math display">\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \frac{\boldsymbol{m}^{(t)}}{\sqrt{\boldsymbol{v}^{(t)}} + \epsilon}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{g}^{(t)} = \nabla_{\boldsymbol{\beta}} \text{RSS}(\boldsymbol{\beta}^{(t)})\)</span></p>
<p><strong>Advantages</strong>: - Works well with default parameters (<span class="math inline">\(\beta_1 = 0.9\)</span>, <span class="math inline">\(\beta_2 = 0.999\)</span>, <span class="math inline">\(\eta = 0.001\)</span>) - Adapts learning rate for each parameter - Widely used in deep learning</p>
</section>
<section id="line-search" class="level3" data-number="17.5.4">
<h3 data-number="17.5.4" class="anchored" data-anchor-id="line-search"><span class="header-section-number">17.5.4</span> 4. Line Search</h3>
<p>At each iteration, optimize the learning rate: <span class="math display">\[
\eta_t = \arg\min_{\eta} \text{RSS}(\boldsymbol{\beta}^{(t)} - \eta \nabla_{\boldsymbol{\beta}} \text{RSS}(\boldsymbol{\beta}^{(t)}))
\]</span></p>
<p><strong>Benefits</strong>: Guaranteed improvement at each step</p>
<p><strong>Cost</strong>: Additional computation per iteration</p>
</section>
</section>
</section>
<section id="statistical-properties-assumptions" class="level1" data-number="18">
<h1 data-number="18"><span class="header-section-number">18</span> Statistical Properties: Assumptions</h1>
<p>The theoretical properties of linear regression rely on several key assumptions. Understanding these helps us know when the method will work well and when it might fail.</p>
<section id="classical-linear-regression-assumptions" class="level2" data-number="18.1">
<h2 data-number="18.1" class="anchored" data-anchor-id="classical-linear-regression-assumptions"><span class="header-section-number">18.1</span> Classical Linear Regression Assumptions</h2>
<section id="linearity" class="level3" data-number="18.1.1">
<h3 data-number="18.1.1" class="anchored" data-anchor-id="linearity"><span class="header-section-number">18.1.1</span> 1. Linearity</h3>
<p><strong>Assumption</strong>: The true relationship is linear in parameters: <span class="math display">\[
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}_{\text{true}} + \boldsymbol{\epsilon}
\]</span></p>
<p><strong>What it means</strong>: There exist true parameters <span class="math inline">\(\boldsymbol{\beta}_{\text{true}}\)</span> such that the data-generating process follows this form.</p>
<p><strong>If violated</strong>: - OLS estimates are biased - Predictions may be systematically wrong - <strong>Solution</strong>: Transform features, add polynomial or interaction terms, use nonlinear models</p>
</section>
<section id="independence" class="level3" data-number="18.1.2">
<h3 data-number="18.1.2" class="anchored" data-anchor-id="independence"><span class="header-section-number">18.1.2</span> 2. Independence</h3>
<p><strong>Assumption</strong>: Samples <span class="math inline">\((\boldsymbol{x}_i, y_i)\)</span> are independent and identically distributed (i.i.d.).</p>
<p><strong>What it means</strong>: - Each observation is drawn independently from the same distribution - Knowing one sample doesn’t tell you about others</p>
<p><strong>If violated</strong>: - Time series data (autocorrelation) - Spatial data (neighboring points are similar) - Clustered data (measurements from same aircraft)</p>
<p><strong>Consequences</strong>: - Standard errors are wrong - Confidence intervals unreliable - <strong>Solution</strong>: Use time series models, spatial models, clustered standard errors</p>
</section>
<section id="homoscedasticity-constant-variance" class="level3" data-number="18.1.3">
<h3 data-number="18.1.3" class="anchored" data-anchor-id="homoscedasticity-constant-variance"><span class="header-section-number">18.1.3</span> 3. Homoscedasticity (Constant Variance)</h3>
<p><strong>Assumption</strong>: Error variance is constant across all observations: <span class="math display">\[
\text{Var}(\epsilon_i) = \sigma^2 \quad \text{for all } i
\]</span></p>
<p><strong>What it means</strong>: The “noise level” is the same regardless of feature values.</p>
<p><strong>Heteroscedasticity (violation)</strong>: Variance depends on features <span class="math display">\[
\text{Var}(\epsilon_i) = \sigma_i^2 \quad \text{(different for each } i \text{)}
\]</span></p>
<p><strong>Aerospace example</strong>: - Sensor noise may increase with dynamic pressure - Measurement error in drag coefficient may depend on angle of attack - Flow unsteadiness increases near stall</p>
<p><strong>If violated</strong>: - OLS is still unbiased but not optimal (not minimum variance) - Standard errors are wrong - <strong>Solution</strong>: Weighted least squares, robust standard errors, transform the response</p>
</section>
<section id="normality" class="level3" data-number="18.1.4">
<h3 data-number="18.1.4" class="anchored" data-anchor-id="normality"><span class="header-section-number">18.1.4</span> 4. Normality</h3>
<p><strong>Assumption</strong>: Errors are normally distributed: <span class="math display">\[
\epsilon_i \sim \mathcal{N}(0, \sigma^2) \quad \text{independently}
\]</span></p>
<p><strong>What it means</strong>: Random fluctuations follow a bell curve (Gaussian distribution).</p>
<p><strong>Importance</strong>: - Required for exact hypothesis tests and confidence intervals - <strong>Not required</strong> for OLS estimation itself - By Central Limit Theorem, normality becomes less critical for large <span class="math inline">\(n\)</span></p>
<p><strong>If violated</strong>: - OLS estimates still unbiased - Hypothesis tests may be unreliable (especially for small <span class="math inline">\(n\)</span>) - <strong>Solution</strong>: Use robust inference, bootstrap, or transform data</p>
</section>
<section id="no-perfect-multicollinearity" class="level3" data-number="18.1.5">
<h3 data-number="18.1.5" class="anchored" data-anchor-id="no-perfect-multicollinearity"><span class="header-section-number">18.1.5</span> 5. No Perfect Multicollinearity</h3>
<p><strong>Assumption</strong>: Columns of <span class="math inline">\(\boldsymbol{X}\)</span> are linearly independent (full rank): <span class="math display">\[
\text{rank}(\boldsymbol{X}) = d + 1
\]</span></p>
<p><strong>What it means</strong>: No feature can be perfectly predicted by other features.</p>
<p><strong>Perfect multicollinearity examples</strong>: - Including both Celsius and Fahrenheit temperature - Including velocity twice - Sum of dummy variables equals 1 (dummy variable trap)</p>
<p><strong>Near multicollinearity</strong>: Features are highly (but not perfectly) correlated - Mach number and true airspeed (at constant altitude/temperature) - Multiple polynomial terms of the same variable - Multiple similar wind tunnel configurations</p>
<p><strong>Consequences</strong>: - <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> is singular or nearly singular - Cannot compute <span class="math inline">\((\boldsymbol{X}^T\boldsymbol{X})^{-1}\)</span> or it’s numerically unstable - Coefficients have huge standard errors - Small data changes cause large coefficient changes</p>
<p><strong>Solutions</strong>: - Remove redundant features - Regularization (Ridge, Lasso) - Principal Component Analysis (PCA) - Domain knowledge to select meaningful features</p>
</section>
</section>
</section>
<section id="gauss-markov-theorem-implications-for-practice" class="level1" data-number="19">
<h1 data-number="19"><span class="header-section-number">19</span> Gauss-Markov Theorem: Implications for Practice</h1>
<section id="statement-of-the-theorem" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="statement-of-the-theorem"><span class="header-section-number">19.1</span> Statement of the Theorem</h2>
<p><strong>Gauss-Markov Theorem</strong>: Under assumptions 1-3 (linearity, independence, homoscedasticity), the OLS estimator <span class="math inline">\(\boldsymbol{\beta}^\ast = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}\)</span> is <strong>BLUE</strong> (Best Linear Unbiased Estimator).</p>
</section>
<section id="what-does-blue-mean" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="what-does-blue-mean"><span class="header-section-number">19.2</span> What Does BLUE Mean?</h2>
<section id="best" class="level3" data-number="19.2.1">
<h3 data-number="19.2.1" class="anchored" data-anchor-id="best"><span class="header-section-number">19.2.1</span> Best</h3>
<p><strong>Minimum variance</strong> among all linear unbiased estimators.</p>
<p><strong>What it means</strong>: - OLS gives the most precise estimates possible (smallest uncertainty) - No other linear unbiased method produces tighter confidence intervals - In the space of linear unbiased estimators, OLS is optimal</p>
<p><strong>Why it matters</strong>: For safety-critical aerospace applications, we want the most precise performance predictions possible.</p>
</section>
<section id="linear" class="level3" data-number="19.2.2">
<h3 data-number="19.2.2" class="anchored" data-anchor-id="linear"><span class="header-section-number">19.2.2</span> Linear</h3>
<p><strong>Estimator is a linear function</strong> of the observations <span class="math inline">\(\boldsymbol{y}\)</span>: <span class="math display">\[
\boldsymbol{\beta}^\ast = \boldsymbol{C}\boldsymbol{y}
\]</span></p>
<p>for some matrix <span class="math inline">\(\boldsymbol{C}\)</span> (that may depend on <span class="math inline">\(\boldsymbol{X}\)</span> but not on <span class="math inline">\(\boldsymbol{y}\)</span>).</p>
<p><strong>For OLS</strong>: <span class="math inline">\(\boldsymbol{C} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\)</span></p>
<p><strong>Why restrict to linear estimators?</strong>: - Computationally tractable - Well-understood statistical properties - Note: Nonlinear estimators might have lower variance, but they’re typically biased</p>
</section>
<section id="unbiased" class="level3" data-number="19.2.3">
<h3 data-number="19.2.3" class="anchored" data-anchor-id="unbiased"><span class="header-section-number">19.2.3</span> Unbiased</h3>
<p><strong>Expected value equals true parameter</strong>: <span class="math display">\[
E[\boldsymbol{\beta}^\ast] = \boldsymbol{\beta}_{\text{true}}
\]</span></p>
<p><strong>What it means</strong>: - On average (over many datasets), estimates equal true values - No systematic over- or under-estimation - If we repeat the experiment many times, the average of our estimates converges to the truth</p>
<p><strong>Why it matters</strong>: We want methods that are correct on average, not systematically biased.</p>
</section>
</section>
<section id="aerospace-context" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="aerospace-context"><span class="header-section-number">19.3</span> Aerospace Context</h2>
<p><strong>Critical for certification</strong>:</p>
<p>Flight envelopes must be determined with: - <strong>Minimal uncertainty</strong> (Best) - <strong>No systematic bias</strong> (Unbiased)</p>
<p>The BLUE property ensures: - Tightest possible bounds on performance predictions - Maximum confidence in safety margins - Regulatory compliance (FAA, EASA require unbiased, minimum-variance estimates)</p>
<p><strong>Example</strong>: When certifying maximum operating Mach number, we need the most precise drag predictions possible. Using OLS (under appropriate assumptions) guarantees we’re using the optimal estimator.</p>
</section>
<section id="when-blue-doesnt-apply" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="when-blue-doesnt-apply"><span class="header-section-number">19.4</span> When BLUE Doesn’t Apply</h2>
<p>BLUE only applies under assumptions 1-3. If:</p>
<ul>
<li><strong>Nonlinearity</strong>: True relationship is nonlinear → OLS is biased</li>
<li><strong>Heteroscedasticity</strong>: Variance is not constant → OLS is unbiased but not minimum variance (use weighted least squares)</li>
<li><strong>Dependence</strong>: Samples are correlated → Standard errors are wrong (use robust methods)</li>
</ul>
<p>Additionally, BLUE is restricted to <strong>linear unbiased</strong> estimators. In modern machine learning: - We often accept <strong>bias</strong> for reduced <strong>variance</strong> (bias-variance tradeoff) - Regularization (Ridge, Lasso) introduces bias but reduces variance - For prediction, biased estimators can outperform OLS</p>
</section>
</section>
<section id="statistical-properties-distribution" class="level1" data-number="20">
<h1 data-number="20"><span class="header-section-number">20</span> Statistical Properties: Distribution</h1>
<p>Understanding the distribution of our parameter estimates allows us to quantify uncertainty and perform statistical inference.</p>
<section id="distribution-of-the-ols-estimator" class="level2" data-number="20.1">
<h2 data-number="20.1" class="anchored" data-anchor-id="distribution-of-the-ols-estimator"><span class="header-section-number">20.1</span> Distribution of the OLS Estimator</h2>
<p><strong>Under the normality assumption</strong> (<span class="math inline">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span> independently), the OLS estimator has a multivariate normal distribution:</p>
<p><span class="math display">\[
\boldsymbol{\beta}^\ast \sim \mathcal{N}\left(\boldsymbol{\beta}_{\text{true}}, \sigma^2(\boldsymbol{X}^T\boldsymbol{X})^{-1}\right)
\]</span></p>
</section>
<section id="unpacking-this-result" class="level2" data-number="20.2">
<h2 data-number="20.2" class="anchored" data-anchor-id="unpacking-this-result"><span class="header-section-number">20.2</span> Unpacking This Result</h2>
<section id="mean-expected-value" class="level3" data-number="20.2.1">
<h3 data-number="20.2.1" class="anchored" data-anchor-id="mean-expected-value"><span class="header-section-number">20.2.1</span> Mean (Expected Value)</h3>
<p><span class="math display">\[
E[\boldsymbol{\beta}^\ast] = \boldsymbol{\beta}_{\text{true}}
\]</span></p>
<p><strong>Unbiasedness</strong>: On average, our estimates equal the true parameters.</p>
</section>
<section id="covariance-matrix" class="level3" data-number="20.2.2">
<h3 data-number="20.2.2" class="anchored" data-anchor-id="covariance-matrix"><span class="header-section-number">20.2.2</span> Covariance Matrix</h3>
<p><span class="math display">\[
\text{Cov}(\boldsymbol{\beta}^\ast) = \sigma^2(\boldsymbol{X}^T\boldsymbol{X})^{-1}
\]</span></p>
<p><strong>What this tells us</strong>:</p>
<ol type="1">
<li><p><strong>Diagonal elements</strong> give the variance of each coefficient: <span class="math display">\[
\text{Var}(\beta_j^\ast) = \sigma^2[(\boldsymbol{X}^T\boldsymbol{X})^{-1}]_{jj}
\]</span></p></li>
<li><p><strong>Off-diagonal elements</strong> show correlations between coefficient estimates: <span class="math display">\[
\text{Cov}(\beta_j^\ast, \beta_k^\ast) = \sigma^2[(\boldsymbol{X}^T\boldsymbol{X})^{-1}]_{jk}
\]</span></p></li>
<li><p><strong>Depends on <span class="math inline">\(\boldsymbol{X}\)</span></strong>: The design of our experiment (which features, which samples) affects estimation precision</p></li>
<li><p><strong>Depends on <span class="math inline">\(\sigma^2\)</span></strong>: More noise → more uncertainty</p></li>
</ol>
</section>
</section>
<section id="practical-implications" class="level2" data-number="20.3">
<h2 data-number="20.3" class="anchored" data-anchor-id="practical-implications"><span class="header-section-number">20.3</span> Practical Implications</h2>
<p>This distribution allows us to:</p>
<ol type="1">
<li><strong>Construct confidence intervals</strong> for each coefficient</li>
<li><strong>Perform hypothesis tests</strong> about parameters</li>
<li><strong>Quantify uncertainty</strong> in predictions</li>
<li><strong>Design experiments</strong> to minimize variance (optimal experimental design)</li>
</ol>
</section>
<section id="what-if-normality-doesnt-hold" class="level2" data-number="20.4">
<h2 data-number="20.4" class="anchored" data-anchor-id="what-if-normality-doesnt-hold"><span class="header-section-number">20.4</span> What If Normality Doesn’t Hold?</h2>
<p><strong>Good news</strong>: Even without normality: - OLS estimates remain <strong>unbiased</strong> (under linearity, independence) - OLS is still <strong>BLUE</strong> (under homoscedasticity) - <strong>Asymptotically</strong> (large <span class="math inline">\(n\)</span>), the Central Limit Theorem implies approximate normality</p>
<p><strong>When it matters</strong>: - Small samples (<span class="math inline">\(n &lt; 30\)</span>): normality important for exact inference - Large samples (<span class="math inline">\(n &gt; 100\)</span>): approximate normality holds regardless - <strong>Solution for small samples</strong>: Use robust inference methods, bootstrap</p>
</section>
</section>
<section id="estimating-the-noise-variance" class="level1" data-number="21">
<h1 data-number="21"><span class="header-section-number">21</span> Estimating the Noise Variance</h1>
<p>In practice, we don’t know the true noise variance <span class="math inline">\(\sigma^2\)</span>. We must estimate it from the data.</p>
<section id="residual-variance-estimator" class="level2" data-number="21.1">
<h2 data-number="21.1" class="anchored" data-anchor-id="residual-variance-estimator"><span class="header-section-number">21.1</span> Residual Variance Estimator</h2>
<p><span class="math display">\[
\hat{\sigma}^2 = \frac{1}{n - d - 1}\sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{\text{RSS}}{n - d - 1}
\]</span></p>
<p>where: - <span class="math inline">\(\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2\)</span> is the residual sum of squares - <span class="math inline">\(n\)</span> is the number of samples - <span class="math inline">\(d\)</span> is the number of features (not counting intercept) - <span class="math inline">\(d + 1\)</span> is the total number of parameters (including intercept)</p>
</section>
<section id="why-divide-by-n---d---1" class="level2" data-number="21.2">
<h2 data-number="21.2" class="anchored" data-anchor-id="why-divide-by-n---d---1"><span class="header-section-number">21.2</span> Why Divide by <span class="math inline">\(n - d - 1\)</span>?</h2>
<section id="degrees-of-freedom" class="level3" data-number="21.2.1">
<h3 data-number="21.2.1" class="anchored" data-anchor-id="degrees-of-freedom"><span class="header-section-number">21.2.1</span> Degrees of Freedom</h3>
<p><strong>Definition</strong>: Degrees of freedom (df) = number of independent pieces of information available for estimating variance.</p>
<p><strong>Calculation</strong>: - Start with <span class="math inline">\(n\)</span> observations - Estimate <span class="math inline">\(d + 1\)</span> parameters (<span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_d\)</span>) - Each parameter estimate “uses up” one degree of freedom - Remaining df: <span class="math inline">\(n - (d + 1) = n - d - 1\)</span></p>
</section>
<section id="why-not-divide-by-n" class="level3" data-number="21.2.2">
<h3 data-number="21.2.2" class="anchored" data-anchor-id="why-not-divide-by-n"><span class="header-section-number">21.2.2</span> Why Not Divide by <span class="math inline">\(n\)</span>?</h3>
<p>If we divided by <span class="math inline">\(n\)</span>, the estimator would be <strong>biased</strong>: <span class="math display">\[
E\left[\frac{1}{n}\sum_{i=1}^n r_i^2\right] &lt; \sigma^2
\]</span></p>
<p>This <strong>underestimates</strong> the true variance because the residuals are constrained to satisfy the normal equations (they’re not truly independent).</p>
<p>Dividing by <span class="math inline">\(n - d - 1\)</span> corrects this bias: <span class="math display">\[
E[\hat{\sigma}^2] = E\left[\frac{\text{RSS}}{n - d - 1}\right] = \sigma^2
\]</span></p>
<p>This makes <span class="math inline">\(\hat{\sigma}^2\)</span> an <strong>unbiased estimator</strong> of <span class="math inline">\(\sigma^2\)</span>.</p>
</section>
</section>
<section id="aerospace-example-1" class="level2" data-number="21.3">
<h2 data-number="21.3" class="anchored" data-anchor-id="aerospace-example-1"><span class="header-section-number">21.3</span> Aerospace Example</h2>
<p><strong>Scenario</strong>: Fit drag coefficient model: <span class="math display">\[
C_D = \beta_0 + \beta_1 \alpha + \beta_2 \alpha^2
\]</span></p>
<p>using <span class="math inline">\(n = 20\)</span> wind tunnel data points.</p>
<p><strong>Parameters</strong>: - Number of features: <span class="math inline">\(d = 2\)</span> (we have <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\alpha^2\)</span>) - Total parameters: <span class="math inline">\(d + 1 = 3\)</span> (including intercept <span class="math inline">\(\beta_0\)</span>) - Degrees of freedom: <span class="math inline">\(n - d - 1 = 20 - 2 - 1 = 17\)</span></p>
<p><strong>Variance estimate</strong>: <span class="math display">\[
\hat{\sigma}^2 = \frac{\sum_{i=1}^{20}(C_{D_i} - \hat{C}_{D_i})^2}{17}
\]</span></p>
</section>
<section id="relationship-to-model-fit" class="level2" data-number="21.4">
<h2 data-number="21.4" class="anchored" data-anchor-id="relationship-to-model-fit"><span class="header-section-number">21.4</span> Relationship to Model Fit</h2>
<p><strong>Small <span class="math inline">\(\hat{\sigma}^2\)</span></strong>: - Residuals are small - Model fits data well - Low prediction uncertainty</p>
<p><strong>Large <span class="math inline">\(\hat{\sigma}^2\)</span></strong>: - Residuals are large - Model doesn’t fit well or data is very noisy - High prediction uncertainty</p>
</section>
</section>
<section id="confidence-intervals-for-coefficients" class="level1" data-number="22">
<h1 data-number="22"><span class="header-section-number">22</span> Confidence Intervals for Coefficients</h1>
<section id="understanding-coefficient-uncertainty" class="level2" data-number="22.1">
<h2 data-number="22.1" class="anchored" data-anchor-id="understanding-coefficient-uncertainty"><span class="header-section-number">22.1</span> Understanding Coefficient Uncertainty</h2>
<p>Each estimated coefficient <span class="math inline">\(\beta_j^\ast\)</span> is a <strong>random variable</strong>—it depends on the random sample we collected. If we repeated the experiment, we’d get different data and different estimates.</p>
<p><strong>Key question</strong>: How uncertain are we about each coefficient?</p>
</section>
<section id="standard-error" class="level2" data-number="22.2">
<h2 data-number="22.2" class="anchored" data-anchor-id="standard-error"><span class="header-section-number">22.2</span> Standard Error</h2>
<p>The <strong>standard error</strong> of <span class="math inline">\(\beta_j^\ast\)</span> measures its sampling variability:</p>
<p><span class="math display">\[
\text{SE}(\beta_j^\ast) = \sqrt{\hat{\sigma}^2 \left[(\boldsymbol{X}^T\boldsymbol{X})^{-1}\right]_{jj}}
\]</span></p>
<p><strong>Interpretation</strong>: - Small SE → precise estimate (high confidence) - Large SE → imprecise estimate (low confidence)</p>
<p><strong>Factors affecting SE</strong>: 1. <strong>Data noise</strong> (<span class="math inline">\(\hat{\sigma}^2\)</span>): More noise → larger SE 2. <strong>Sample size</strong> (<span class="math inline">\(n\)</span>): More data → smaller SE (typically <span class="math inline">\(\text{SE} \propto 1/\sqrt{n}\)</span>) 3. <strong>Feature correlation</strong>: Correlated features → larger SE 4. <strong>Feature variance</strong>: Features with more spread → smaller SE</p>
</section>
<section id="confidence-interval-formula" class="level2" data-number="22.3">
<h2 data-number="22.3" class="anchored" data-anchor-id="confidence-interval-formula"><span class="header-section-number">22.3</span> Confidence Interval Formula</h2>
<p>A <strong><span class="math inline">\(100(1-\gamma)\%\)</span> confidence interval</strong> for <span class="math inline">\(\beta_j\)</span> is:</p>
<p><span class="math display">\[
\beta_j^\ast \pm t_{\gamma/2, n-d-1} \cdot \text{SE}(\beta_j^\ast)
\]</span></p>
<p>where <span class="math inline">\(t_{\gamma/2, n-d-1}\)</span> is the critical value from the <strong>t-distribution</strong> with <span class="math inline">\(n-d-1\)</span> degrees of freedom.</p>
<p><strong>Common confidence levels</strong>: - 95% confidence: <span class="math inline">\(\gamma = 0.05\)</span>, so <span class="math inline">\(t_{0.025, n-d-1}\)</span> - 90% confidence: <span class="math inline">\(\gamma = 0.10\)</span>, so <span class="math inline">\(t_{0.05, n-d-1}\)</span> - 99% confidence: <span class="math inline">\(\gamma = 0.01\)</span>, so <span class="math inline">\(t_{0.005, n-d-1}\)</span></p>
<p><strong>For large <span class="math inline">\(n\)</span></strong>: t-distribution approaches normal, <span class="math inline">\(t_{\gamma/2, n-d-1} \approx z_{\gamma/2}\)</span> - 95% CI: <span class="math inline">\(z_{0.025} \approx 1.96 \approx 2\)</span> - 90% CI: <span class="math inline">\(z_{0.05} \approx 1.645\)</span></p>
</section>
<section id="interpretation" class="level2" data-number="22.4">
<h2 data-number="22.4" class="anchored" data-anchor-id="interpretation"><span class="header-section-number">22.4</span> Interpretation</h2>
<p><strong>What a 95% CI means</strong>:</p>
<blockquote class="blockquote">
<p>If we repeated the experiment many times and computed a 95% CI each time, approximately 95% of these intervals would contain the true parameter value <span class="math inline">\(\beta_j\)</span>.</p>
</blockquote>
<p><strong>What it does NOT mean</strong>: - The probability that <span class="math inline">\(\beta_j\)</span> is in this interval is 95% (Bayesian interpretation) - We are 95% sure our estimate is correct</p>
</section>
<section id="aerospace-example-2" class="level2" data-number="22.5">
<h2 data-number="22.5" class="anchored" data-anchor-id="aerospace-example-2"><span class="header-section-number">22.5</span> Aerospace Example</h2>
<p><strong>Scenario</strong>: Fitting lift coefficient model: <span class="math display">\[
C_L = \beta_0 + \beta_1 \alpha
\]</span></p>
<p><strong>Results</strong>: - <span class="math inline">\(\beta_1^\ast = 0.105\)</span> (per degree) - <span class="math inline">\(\text{SE}(\beta_1^\ast) = 0.004\)</span> - <span class="math inline">\(n = 30\)</span>, <span class="math inline">\(d = 1\)</span>, so df <span class="math inline">\(= 28\)</span> - <span class="math inline">\(t_{0.025, 28} \approx 2.048\)</span></p>
<p><strong>95% Confidence Interval</strong>: <span class="math display">\[
0.105 \pm 2.048 \times 0.004 = 0.105 \pm 0.008 = [0.097, 0.113]
\]</span></p>
<p><strong>Interpretation</strong>: We are 95% confident the true lift slope is between 0.097 and 0.113 per degree.</p>
<p><strong>Engineering decision</strong>: This precision allows us to set conservative performance bounds for flight envelope calculations.</p>
</section>
</section>
<section id="hypothesis-testing-for-individual-coefficients" class="level1" data-number="23">
<h1 data-number="23"><span class="header-section-number">23</span> Hypothesis Testing for Individual Coefficients</h1>
<section id="the-central-question" class="level2" data-number="23.1">
<h2 data-number="23.1" class="anchored" data-anchor-id="the-central-question"><span class="header-section-number">23.1</span> The Central Question</h2>
<p>After estimating coefficients from data, we ask: <strong>Is this coefficient significantly different from zero, or could it just be noise?</strong></p>
<p>This is crucial for: - <strong>Feature selection</strong>: Which features actually matter? - <strong>Model simplification</strong>: Can we remove unimportant features? - <strong>Physical interpretation</strong>: Is this effect real or spurious?</p>
</section>
<section id="setting-up-the-test" class="level2" data-number="23.2">
<h2 data-number="23.2" class="anchored" data-anchor-id="setting-up-the-test"><span class="header-section-number">23.2</span> Setting Up the Test</h2>
<section id="hypotheses" class="level3" data-number="23.2.1">
<h3 data-number="23.2.1" class="anchored" data-anchor-id="hypotheses"><span class="header-section-number">23.2.1</span> Hypotheses</h3>
<p><strong>Null hypothesis</strong> <span class="math inline">\(H_0\)</span>: <span class="math display">\[
\beta_j = 0
\]</span></p>
<p><strong>Interpretation</strong>: Feature <span class="math inline">\(j\)</span> has <strong>no true effect</strong> on the response. Any non-zero estimate we observed is just random sampling variation.</p>
<p><strong>Alternative hypothesis</strong> <span class="math inline">\(H_1\)</span>: <span class="math display">\[
\beta_j \neq 0
\]</span></p>
<p><strong>Interpretation</strong>: Feature <span class="math inline">\(j\)</span> <strong>does affect</strong> the response. The estimated effect is real, not just noise.</p>
</section>
</section>
<section id="the-test-statistic" class="level2" data-number="23.3">
<h2 data-number="23.3" class="anchored" data-anchor-id="the-test-statistic"><span class="header-section-number">23.3</span> The Test Statistic</h2>
<p>We construct a <strong>t-statistic</strong>:</p>
<p><span class="math display">\[
t_j = \frac{\beta_j^\ast}{\text{SE}(\beta_j^\ast)} = \frac{\text{Estimated coefficient}}{\text{Standard error of estimate}}
\]</span></p>
<p><strong>Interpretation</strong>: - Measures <strong>how many standard errors</strong> the estimate is away from zero - If <span class="math inline">\(\beta_j = 0\)</span> truly, we expect <span class="math inline">\(\beta_j^\ast \approx 0\)</span> (within sampling error) - Large <span class="math inline">\(|t_j|\)</span> means the estimate is far from zero → unlikely if <span class="math inline">\(H_0\)</span> is true</p>
</section>
<section id="distribution-under-the-null" class="level2" data-number="23.4">
<h2 data-number="23.4" class="anchored" data-anchor-id="distribution-under-the-null"><span class="header-section-number">23.4</span> Distribution Under the Null</h2>
<p><strong>If <span class="math inline">\(H_0\)</span> is true</strong> (<span class="math inline">\(\beta_j = 0\)</span>), then:</p>
<p><span class="math display">\[
t_j \sim t_{n-d-1}
\]</span></p>
<p>The test statistic follows a <strong>t-distribution</strong> with <span class="math inline">\(n - d - 1\)</span> degrees of freedom.</p>
<p>This allows us to compute <strong>p-values</strong> and make decisions.</p>
</section>
<section id="making-the-decision" class="level2" data-number="23.5">
<h2 data-number="23.5" class="anchored" data-anchor-id="making-the-decision"><span class="header-section-number">23.5</span> Making the Decision</h2>
<section id="approach-1-p-value" class="level3" data-number="23.5.1">
<h3 data-number="23.5.1" class="anchored" data-anchor-id="approach-1-p-value"><span class="header-section-number">23.5.1</span> Approach 1: P-value</h3>
<p><strong>P-value</strong>: The probability of observing a test statistic as extreme as (or more extreme than) what we got, if the null hypothesis were true.</p>
<p><span class="math display">\[
p\text{-value} = P(|t| \geq |t_j| \mid H_0 \text{ is true})
\]</span></p>
<p><strong>Decision rule</strong>: - If <span class="math inline">\(p &lt; \alpha\)</span> (significance level, typically 0.05): <strong>Reject <span class="math inline">\(H_0\)</span></strong> (coefficient is significant) - If <span class="math inline">\(p \geq \alpha\)</span>: <strong>Fail to reject <span class="math inline">\(H_0\)</span></strong> (insufficient evidence that coefficient differs from zero)</p>
<p><strong>Interpretation of p-values</strong>: - <span class="math inline">\(p &lt; 0.001\)</span>: Very strong evidence against <span class="math inline">\(H_0\)</span> (highly significant) - <span class="math inline">\(p &lt; 0.01\)</span>: Strong evidence - <span class="math inline">\(p &lt; 0.05\)</span>: Moderate evidence (conventional cutoff) - <span class="math inline">\(p &gt; 0.05\)</span>: Weak or no evidence</p>
</section>
<section id="approach-2-critical-value" class="level3" data-number="23.5.2">
<h3 data-number="23.5.2" class="anchored" data-anchor-id="approach-2-critical-value"><span class="header-section-number">23.5.2</span> Approach 2: Critical Value</h3>
<p><strong>Critical value</strong>: The threshold <span class="math inline">\(t_{\text{crit}} = t_{\alpha/2, n-d-1}\)</span> such that:</p>
<p><span class="math display">\[
P(|t| &gt; t_{\text{crit}} \mid H_0) = \alpha
\]</span></p>
<p><strong>Decision rule</strong>: - If <span class="math inline">\(|t_j| &gt; t_{\text{crit}}\)</span>: <strong>Reject <span class="math inline">\(H_0\)</span></strong> - If <span class="math inline">\(|t_j| \leq t_{\text{crit}}\)</span>: <strong>Fail to reject <span class="math inline">\(H_0\)</span></strong></p>
<p><strong>Equivalence</strong>: Both approaches give the same conclusion. The p-value approach is more informative because it quantifies the strength of evidence.</p>
</section>
</section>
<section id="aerospace-example-3" class="level2" data-number="23.6">
<h2 data-number="23.6" class="anchored" data-anchor-id="aerospace-example-3"><span class="header-section-number">23.6</span> Aerospace Example</h2>
<p><strong>Scenario</strong>: Testing whether Reynolds number affects drag coefficient in our model:</p>
<p><span class="math display">\[
C_D = \beta_0 + \beta_1 \alpha + \beta_2 \alpha^2 + \beta_3 \log(\text{Re})
\]</span></p>
<p><strong>Results for <span class="math inline">\(\beta_3\)</span></strong>: - <span class="math inline">\(\beta_3^\ast = -0.0015\)</span> - <span class="math inline">\(\text{SE}(\beta_3^\ast) = 0.0008\)</span> - <span class="math inline">\(n = 50\)</span>, <span class="math inline">\(d = 3\)</span>, df <span class="math inline">\(= 46\)</span></p>
<p><strong>Test statistic</strong>: <span class="math display">\[
t_3 = \frac{-0.0015}{0.0008} = -1.875
\]</span></p>
<p><strong>P-value</strong> (two-tailed, df = 46): <span class="math display">\[
p \approx 0.067
\]</span></p>
<p><strong>Decision at <span class="math inline">\(\alpha = 0.05\)</span></strong>: - <span class="math inline">\(p = 0.067 &gt; 0.05\)</span>: Fail to reject <span class="math inline">\(H_0\)</span> - <strong>Conclusion</strong>: Insufficient evidence that Reynolds number significantly affects drag (at 5% significance level)</p>
<p><strong>Engineering interpretation</strong>: - We might consider <strong>removing</strong> <span class="math inline">\(\log(\text{Re})\)</span> from the model to simplify it - Or acknowledge that Reynolds number effects are weak in our data range - Or collect more data to increase statistical power</p>
</section>
<section id="important-caveats" class="level2" data-number="23.7">
<h2 data-number="23.7" class="anchored" data-anchor-id="important-caveats"><span class="header-section-number">23.7</span> Important Caveats</h2>
<ol type="1">
<li><p><strong>Statistical significance ≠ practical significance</strong>: A tiny effect can be statistically significant with enough data</p></li>
<li><p><strong>Multiple testing</strong>: Testing many coefficients increases false positive rate (use Bonferroni correction or similar)</p></li>
<li><p><strong>Correlation</strong>: In models with many correlated features, individual t-tests can be misleading</p></li>
<li><p><strong>Model assumptions</strong>: These tests assume normality, homoscedasticity, etc.</p></li>
</ol>
</section>
</section>
<section id="summary" class="level1" data-number="24">
<h1 data-number="24"><span class="header-section-number">24</span> Summary</h1>
<p>Linear regression is a powerful and versatile tool for aerospace engineering applications. Key takeaways:</p>
<section id="mathematical-foundations" class="level2" data-number="24.1">
<h2 data-number="24.1" class="anchored" data-anchor-id="mathematical-foundations"><span class="header-section-number">24.1</span> Mathematical Foundations</h2>
<ul>
<li>Linear regression models relationships as <span class="math inline">\(\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span></li>
<li>“Linear” refers to parameters, not features—we can model nonlinear relationships</li>
<li>The optimal solution minimizes squared error: <span class="math inline">\(\min \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2\)</span></li>
</ul>
</section>
<section id="two-solution-approaches" class="level2" data-number="24.2">
<h2 data-number="24.2" class="anchored" data-anchor-id="two-solution-approaches"><span class="header-section-number">24.2</span> Two Solution Approaches</h2>
<ol type="1">
<li><strong>Closed-form</strong> (Normal Equations): <span class="math inline">\(\boldsymbol{\beta}^\ast = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}\)</span>
<ul>
<li>Exact, one-step solution</li>
<li>Requires matrix inversion</li>
<li>Can fail for large <span class="math inline">\(d\)</span> or singular <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{X}\)</span></li>
</ul></li>
<li><strong>Gradient Descent</strong>: Iterative optimization
<ul>
<li>Works for any problem size</li>
<li>Requires tuning (learning rate, iterations)</li>
<li>Foundation of modern machine learning</li>
</ul></li>
</ol>
</section>
<section id="geometric-insight" class="level2" data-number="24.3">
<h2 data-number="24.3" class="anchored" data-anchor-id="geometric-insight"><span class="header-section-number">24.3</span> Geometric Insight</h2>
<ul>
<li>OLS finds the <strong>orthogonal projection</strong> of <span class="math inline">\(\boldsymbol{y}\)</span> onto col(<span class="math inline">\(\boldsymbol{X}\)</span>)</li>
<li>Residuals are <strong>perpendicular</strong> to feature space: <span class="math inline">\(\boldsymbol{X}^T\boldsymbol{r} = \boldsymbol{0}\)</span></li>
<li>Projection matrix <span class="math inline">\(\boldsymbol{P} = \boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\)</span> separates signal from noise</li>
</ul>
</section>
<section id="statistical-properties" class="level2" data-number="24.4">
<h2 data-number="24.4" class="anchored" data-anchor-id="statistical-properties"><span class="header-section-number">24.4</span> Statistical Properties</h2>
<ul>
<li>Under standard assumptions, OLS is <strong>BLUE</strong> (Best Linear Unbiased Estimator)</li>
<li>Coefficient distribution: <span class="math inline">\(\boldsymbol{\beta}^\ast \sim \mathcal{N}(\boldsymbol{\beta}_{\text{true}}, \sigma^2(\boldsymbol{X}^T\boldsymbol{X})^{-1})\)</span></li>
<li>Enables <strong>confidence intervals</strong> and <strong>hypothesis tests</strong></li>
</ul>
</section>
<section id="practical-considerations" class="level2" data-number="24.5">
<h2 data-number="24.5" class="anchored" data-anchor-id="practical-considerations"><span class="header-section-number">24.5</span> Practical Considerations</h2>
<ul>
<li>Check assumptions (linearity, independence, homoscedasticity, normality)</li>
<li>Watch for multicollinearity</li>
<li>Use appropriate validation metrics</li>
<li>Consider regularization for high-dimensional problems</li>
</ul>
</section>
<section id="aerospace-applications" class="level2" data-number="24.6">
<h2 data-number="24.6" class="anchored" data-anchor-id="aerospace-applications"><span class="header-section-number">24.6</span> Aerospace Applications</h2>
<p>Linear regression is essential for: - Performance prediction (drag, lift, fuel consumption) - Wind tunnel data analysis - Flight test analysis - System identification - Model validation and uncertainty quantification</p>
<p>The techniques you’ve learned form the foundation for more advanced machine learning methods used throughout aerospace engineering.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>